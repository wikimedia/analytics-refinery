#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Note: You should make sure to put refinery/python on your PYTHONPATH.
#   export PYTHONPATH=$PYTHONPATH:/path/to/refinery/python

"""
Automatically drops old partitions from the mediawiki raw and historical
tables, and tables derived from them, that keep the same snapshot structure.
See AFFECTED_TABLES dict for a comprehensive list.

As this data sets are historical (they span from the beginning of time
to latest import), the dimension used to determine which partitions need
to be removed is not time, it's "snapshot". The number of snapshot kept is
6 for all tables except it is 2 for mediawiki_wikitext_history (huge dataset).

Note: Ad-hoc snapshots not following the default naming convention
snapshot=YYYY-MM, like private snapshots, are not considered neither
affected by this script.

Usage: refinery-drop-mediawiki-snapshots [options]

Options:
    -h --help                       Show this help message and exit.
    -v --verbose                    Turn on verbose debug logging.
    -n --dry-run                    Don't actually drop any partitions, just output Hive queries.
    -x --non-strict                 Don't enforce to-be-deleted hive-partitions to match to-be-deleted
                                    folders and vice-versa
"""


from docopt import docopt
from refinery.hive import Hive
from refinery.hdfs import Hdfs
import datetime
import logging
import os
import re
import sys


# Set up logging to be split:
#   INFO+DEBUG+WARNING -> stdout
#   ERROR              -> stderr
# Thanks to https://stackoverflow.com/users/5124424/zoey-greer
class LessThanFilter(logging.Filter):
    def __init__(self, exclusive_maximum, name=""):
        super(LessThanFilter, self).__init__(name)
        self.max_level = exclusive_maximum

    def filter(self, record):
        #non-zero return means we log this message
        return 1 if record.levelno < self.max_level else 0

logger = logging.getLogger()
logger.setLevel(logging.NOTSET)

formatter = logging.Formatter(
    fmt='%(asctime)s %(levelname)-6s %(message)s',
    datefmt='%Y-%m-%dT%H:%M:%S',
)

handler_out = logging.StreamHandler(sys.stdout)
handler_out.setLevel(logging.DEBUG)
handler_out.addFilter(LessThanFilter(logging.ERROR))
handler_out.setFormatter(formatter)
logger.addHandler(handler_out)

handler_err = logging.StreamHandler(sys.stderr)
handler_err.setLevel(logging.ERROR)
handler_err.setFormatter(formatter)
logger.addHandler(handler_err)


# Tables that have mediawiki snapshots to be managed
# key: database, value: table
AFFECTED_TABLES = {
    'wmf_raw': {
        'mediawiki_archive': 6,
        'mediawiki_change_tag': 6,
        'mediawiki_ipblocks': 6,
        'mediawiki_logging': 6,
        'mediawiki_page': 6,
        'mediawiki_pagelinks': 6,
        'mediawiki_project_namespace_map': 6,
        'mediawiki_redirect': 6,
        'mediawiki_revision': 6,
        'mediawiki_user': 6,
        'mediawiki_user_groups': 6
    },
    'wmf': {
        'mediawiki_history': 6,
        'mediawiki_metrics': 6,
        'mediawiki_page_history': 6,
        'mediawiki_user_history': 6,
        'mediawiki_history_reduced': 6,
        'edit_hourly': 6,
        'mediawiki_wikitext_history': 2,
        'mediawiki_wikitext_current': 6,
        'wikidata_entity': 6,
        'wikidata_item_page_link': 6,
    },
    'structured_data': {
        'commons_entity': 6,
    }
}

# Tables partitioned by wiki_db in addition to by snapshot
WIKI_DB_TABLES = [
    'mediawiki_archive',
    'mediawiki_ipblocks',
    'mediawiki_change_tag',
    'mediawiki_logging',
    'mediawiki_page',
    'mediawiki_pagelinks',
    'mediawiki_redirect',
    'mediawiki_revision',
    'mediawiki_user',
    'mediawiki_user_groups',
    'mediawiki_wikitext_history',
    'mediawiki_wikitext_current'
]


# Returns the partitions to be dropped given a hive table
def get_partitions_to_drop(hive, table, keep_snapshots):
    logger.debug('Getting partitions to drop...')
    partitions = hive.partition_specs(table)
    spec_separator = Hive.partition_spec_separator

    # For tables partitioned by dimensions other than snapshot
    # extract just the snapshot spec:
    # snapshot=2017-01,wiki_db=enwiki => snapshot=2017-01
    if table in WIKI_DB_TABLES:
        snapshots = set([])
        for partition in partitions:
            snapshot = partition.split(spec_separator)[0]
            snapshots.add(snapshot)
        partitions = list(snapshots)

    # Filter out ad-hoc or private snapshots
    partitions = [
        p for p in partitions
        if re.match("^snapshot='[0-9]{4}-[0-9]{2}(-[0-9]{2})?'$", p)
    ]

    # Select partitions to drop (keep the most recent <keep_snapshots> ones)
    partitions.sort(reverse = True)
    partitions_to_drop = partitions[keep_snapshots:]

    # HACK: For tables partitioned by dimensions other than snapshot
    # add <dimension>!='' to snapshot spec, so that Hive deletes
    # the whole snapshot partition with all sub-partitions in it.
    if table in WIKI_DB_TABLES:
        partitions_to_drop = [
            spec_separator.join([p, "wiki_db!=''"])
            for p in partitions_to_drop
        ]
    return partitions_to_drop

# Returns the directories to be removed given a hive table
def get_directories_to_remove(hive, table, keep_snapshots):
    logger.debug('Getting directories to remove...')
    table_location = hive.table_location(table)

    # Get partition directories having a _SUCCESS file to only consider fully
    # imported/computed snapshots
    glob = os.path.join(table_location, '*/_SUCCESS')
    directories = Hdfs.ls(glob, include_children=False)

    # Filter out non-date snapshots
    directories = [
        d[:(len(d) - len('/_SUCCESS'))] for d in directories
        if re.match('^.*/snapshot=[0-9]{4}-[0-9]{2}(-[0-9]{2})?/_SUCCESS$', d)
    ]

    # Select directories to drop (keep the most recent <keep_snapshots> ones)
    directories.sort(reverse = True)
    return directories[keep_snapshots:]

# Raises an error if partitions and directories do not match
def check_partitions_vs_directories(partitions, directories):
    spec_separator = Hive.partition_spec_separator
    partition_snapshots = set([p.split(spec_separator)[0].replace("'", '') for p in partitions])
    directory_snapshots = set([os.path.basename(d) for d in directories])
    if partition_snapshots != directory_snapshots:
        logger.error(
            'Selected partitions extracted from table specs ({0}) '
            'does not match selected partitions extracted from data paths ({1}). '
            'HDFS directories to check: {2}'
            .format(partition_snapshots, directory_snapshots, directories)
        )
        sys.exit(1)

# Drop given hive table partitions (if dry_run, just print)
def drop_partitions(hive, table, partitions, dry_run):
    if partitions:
        logger.info(
            'Dropping {0} partitions from {1}.{2}'
            .format(len(partitions), hive.database, table))
        for partition in partitions:
            logger.debug('\t{0}'.format(partition))
        if not dry_run:
            hive.drop_partitions(table, partitions)
    else:
        logger.info(
            'No partitions need to be dropped from {0}.{1}'
            .format(hive.database, table)
        )

# Remove given data directories (if dry_run, just print)
def remove_directories(hive, table, directories, dry_run):
    table_location = hive.table_location(table)
    if directories:
        logger.info('Removing {0} directories from {1}'
            .format(len(directories), table_location))
        for directory in directories:
            logger.debug('\t{0}'.format(directory))
        if not dry_run:
            Hdfs.rm(' '.join(directories))
    else:
        logger.info('No directories need to be removed for {0}'.format(table_location))


if __name__ == '__main__':
    # Parse arguments
    arguments = docopt(__doc__)
    verbose         = arguments['--verbose']
    dry_run         = arguments['--dry-run']
    non_strict      = arguments['--non-strict']

    # Setup logging level
    logger.setLevel(logging.INFO)
    if verbose:
        logger.setLevel(logging.DEBUG)

    logger.debug("Running in VERBOSE mode")
    if dry_run:
        logger.info("Running in DRY-RUN mode")
    if non_strict:
        logger.info("Running in NON_STRICT mode")

    for database, tables_and_keep_snapshots in AFFECTED_TABLES.items():
        # Instantiate Hive
        hive = Hive(database)

        # Apply the cleaning to each table
        for table, keep_snapshot in tables_and_keep_snapshots.items():
            logger.debug('Processing table {0} keeping {1} snapshots'.format(table, keep_snapshot))
            partitions = get_partitions_to_drop(hive, table, keep_snapshot)
            directories = get_directories_to_remove(hive, table, keep_snapshot)
            if not non_strict:
                check_partitions_vs_directories(partitions, directories)
            drop_partitions(hive, table, partitions, dry_run)
            remove_directories(hive, table, directories, dry_run)
