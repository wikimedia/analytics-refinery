#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Note: You should make sure to put refinery/python on your PYTHONPATH.
#   export PYTHONPATH=$PYTHONPATH:/path/to/refinery/python

"""
Automatically drops old partitions from the mediawiki raw and historical
tables, and tables derived from them, that keep the same snapshot structure.
See AFFECTED_TABLES dict for a comprehensive list.

As this data sets are historical (they span from the beginning of time
to latest import), the dimension used to determine which partitions need
to be removed is not time, it's "snapshot". The number of snapshot kept is
6 for all tables except it is 2 for mediawiki_wikitext_history (huge dataset).

Note: Ad-hoc snapshots not following the default naming convention
snapshot=YYYY-MM[-DD], like private snapshots, are not considered neither
affected by this script.

Usage: refinery-drop-mediawiki-snapshots [options]

Options:
    -h --help                       Show this help message and exit.
    -v --verbose                    Turn on verbose debug logging.
    -n --dry-run                    Don't actually drop any partitions, just output Hive queries.
"""


from docopt import docopt
from refinery.hive import Hive, HivePartition
from refinery.hdfs import Hdfs
import datetime
import logging
import os
import re
import sys


# Set up logging to be split:
#   INFO+DEBUG+WARNING -> stdout
#   ERROR              -> stderr
# Thanks to https://stackoverflow.com/users/5124424/zoey-greer
class LessThanFilter(logging.Filter):
    def __init__(self, exclusive_maximum, name=""):
        super(LessThanFilter, self).__init__(name)
        self.max_level = exclusive_maximum

    def filter(self, record):
        #non-zero return means we log this message
        return 1 if record.levelno < self.max_level else 0

logger = logging.getLogger()
logger.setLevel(logging.NOTSET)

formatter = logging.Formatter(
    fmt='%(asctime)s %(levelname)-6s %(message)s',
    datefmt='%Y-%m-%dT%H:%M:%S',
)

handler_out = logging.StreamHandler(sys.stdout)
handler_out.setLevel(logging.DEBUG)
handler_out.addFilter(LessThanFilter(logging.ERROR))
handler_out.setFormatter(formatter)
logger.addHandler(handler_out)

handler_err = logging.StreamHandler(sys.stderr)
handler_err.setLevel(logging.ERROR)
handler_err.setFormatter(formatter)
logger.addHandler(handler_err)


# Tables that have mediawiki snapshots to be managed
# key: database, value: table
AFFECTED_TABLES = {
    'wmf_raw': {
        'mediawiki_archive': 6,
        'mediawiki_category': 6,
        'mediawiki_categorylinks': 6,
        'mediawiki_change_tag_def': 6,
        'mediawiki_change_tag': 6,
        'mediawiki_content_models': 6,
        'mediawiki_content': 6,
        'mediawiki_externallinks': 6,
        'mediawiki_externallinks_old': 6,  # Should be removed completely by 2023-12-01
        'mediawiki_image': 6,
        'mediawiki_imagelinks': 6,
        'mediawiki_ipblocks_restrictions': 6,
        'mediawiki_ipblocks': 6,
        'mediawiki_iwlinks': 6,
        'mediawiki_langlinks': 6,
        'mediawiki_linktarget': 6,
        'mediawiki_logging': 6,
        'mediawiki_page_props': 6,
        'mediawiki_page_restrictions': 6,
        'mediawiki_page': 6,
        'mediawiki_pagelinks': 6,
        'mediawiki_project_namespace_map': 6,
        'mediawiki_redirect': 6,
        'mediawiki_revision': 6,
        'mediawiki_slot_roles': 6,
        'mediawiki_slots': 6,
        'mediawiki_templatelinks': 6,
        'mediawiki_user_groups': 6,
        'mediawiki_user_properties': 6,
        'mediawiki_user': 6,
        'mediawiki_wbc_entity_usage': 6,
        'mediawiki_private_actor': 6,
        'mediawiki_private_comment': 6,
        'mediawiki_private_linktarget': 6,
        'mediawiki_private_watchlist': 6,
        'wikibase_wbt_item_terms': 6,
        'wikibase_wbt_property_terms': 6,
        'wikibase_wbt_term_in_lang': 6,
        'wikibase_wbt_text_in_lang': 6,
        'wikibase_wbt_type': 6,
    },
    'wmf': {
        'mediawiki_history': 6,
        'mediawiki_metrics': 6,
        'mediawiki_page_history': 6,
        'mediawiki_user_history': 6,
        'mediawiki_history_reduced': 6,
        'edit_hourly': 6,
        'mediawiki_wikitext_history': 2,
        'mediawiki_wikitext_current': 6,
        'wikidata_entity': 6,
        'wikidata_item_page_link': 6,
    },
    'structured_data': {
        'commons_entity': 6,
    }
}

# Tables partitioned by wiki_db in addition to by snapshot
WIKI_DB_TABLES = [
    'mediawiki_archive',
    'mediawiki_category',
    'mediawiki_categorylinks',
    'mediawiki_change_tag_def',
    'mediawiki_change_tag',
    'mediawiki_content_models',
    'mediawiki_content',
    'mediawiki_externallinks',
    'mediawiki_image',
    'mediawiki_imagelinks',
    'mediawiki_ipblocks_restrictions',
    'mediawiki_ipblocks',
    'mediawiki_iwlinks',
    'mediawiki_langlinks',
    'mediawiki_linktarget',
    'mediawiki_logging',
    'mediawiki_page_props',
    'mediawiki_page_restrictions',
    'mediawiki_page',
    'mediawiki_pagelinks',
    'mediawiki_redirect',
    'mediawiki_revision',
    'mediawiki_slot_roles',
    'mediawiki_slots',
    'mediawiki_templatelinks',
    'mediawiki_user_groups',
    'mediawiki_user_properties',
    'mediawiki_user',
    'mediawiki_wbc_entity_usage',
    'mediawiki_private_actor',
    'mediawiki_private_comment',
    'mediawiki_private_linktarget',
    'mediawiki_private_watchlist',
    'wikibase_wbt_item_terms',
    'wikibase_wbt_property_terms',
    'wikibase_wbt_term_in_lang',
    'wikibase_wbt_text_in_lang',
    'wikibase_wbt_type',
    'mediawiki_wikitext_history',
    'mediawiki_wikitext_current',
]


# Returns the partitions to be dropped given a hive table
def get_partitions_to_drop(hive, table, keep_snapshots):
    logger.debug('Getting partitions to drop...')
    partitions = hive.partition_specs(table)

    # For tables partitioned by dimensions other than snapshot
    # extract just the snapshot spec:
    # snapshot=2017-01,wiki_db=enwiki => snapshot=2017-01
    if table in WIKI_DB_TABLES:
        snapshots = set([])
        for partition in partitions:
            snapshot = partition.split(Hive.partition_spec_separator)[0]
            snapshots.add(snapshot)
        partitions = list(snapshots)

    # Filter out ad-hoc or private snapshots
    partitions = [
        p for p in partitions
        if re.match("^snapshot='[0-9]{4}-[0-9]{2}(-[0-9]{2})?'$", p)
    ]

    # Select partitions to drop (keep the most recent <keep_snapshots> ones)
    partitions.sort(reverse = True)
    partitions_to_drop = partitions[keep_snapshots:]

    return partitions_to_drop

# Returns the directories to be removed given a hive table
# and its partitions to be dropped
# Note: directories to be removed are computed from table base-path
# joined to the snapshot partition. This script doesn't work
# if the partition-path is set manually to a non hive-formatted location.
def get_directories_to_remove(hive, table, partitions_to_drop):
    logger.debug('Getting directories to remove...')
    table_location = hive.table_location(table)

    return [
        HivePartition(p).path(table_location)
        for p in partitions_to_drop
    ]

# Drop given hive table partitions (if dry_run, just print)
def drop_partitions(hive, table, partitions, dry_run):
    if partitions:
        # HACK: For tables partitioned by dimensions other than snapshot
        # add <dimension>!='' to snapshot spec, so that Hive deletes
        # the whole snapshot partition with all sub-partitions in it.
        if table in WIKI_DB_TABLES:
            partitions = [
                Hive.partition_spec_separator.join([p, "wiki_db!=''"])
                for p in partitions
            ]

        logger.info(
            'Dropping {0} partitions from {1}.{2}'
            .format(len(partitions), hive.database, table))
        for partition in partitions:
            logger.debug('\t{0}'.format(partition))
        if not dry_run:
            hive.drop_partitions(table, partitions)
    else:
        logger.info(
            'No partitions need to be dropped from {0}.{1}'
            .format(hive.database, table)
        )

# Remove given data directories (if dry_run, just print)
def remove_directories(hive, table, directories, dry_run):
    table_location = hive.table_location(table)
    if directories:
        logger.info('Removing {0} directories from {1}'
            .format(len(directories), table_location))
        for directory in directories:
            logger.debug('\t{0}'.format(directory))
        if not dry_run:
            Hdfs.rm(' '.join(directories))
    else:
        logger.info('No directories need to be removed for {0}'.format(table_location))


if __name__ == '__main__':
    # Parse arguments
    arguments = docopt(__doc__)
    verbose         = arguments['--verbose']
    dry_run         = arguments['--dry-run']

    # Setup logging level
    logger.setLevel(logging.INFO)
    if verbose:
        logger.setLevel(logging.DEBUG)
    logger.debug("Running in VERBOSE mode")

    if dry_run:
        logger.info("Running in DRY-RUN mode")

    for database, tables_and_keep_snapshots in AFFECTED_TABLES.items():
        # Instantiate Hive
        hive = Hive(database)

        # Apply the cleaning to each table
        for table, keep_snapshot in tables_and_keep_snapshots.items():
            logger.debug('Processing table {0} keeping {1} snapshots'.format(table, keep_snapshot))
            partitions = get_partitions_to_drop(hive, table, keep_snapshot)
            directories = get_directories_to_remove(hive, table, partitions)
            drop_partitions(hive, table, partitions, dry_run)
            remove_directories(hive, table, directories, dry_run)
