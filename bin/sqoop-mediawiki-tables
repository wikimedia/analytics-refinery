#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Note: needs python3 to run unless we backport concurrent and urllib.parse
#
# Example command that can be run in a cron:
#   export PYTHONPATH=/home/milimetric/refinery/python
#   python3 sqoop-mediawiki-tables \
#     --jdbc-host analytics-store.eqiad.wmnet \
#     --output-dir /user/milimetric/wmf/data/raw/mediawiki/tables \
#     --user research \
#     --password-file /user/milimetric/mysql-analytics-research-client-pw.txt \
#     --wiki-file \
# "/mnt/hdfs/wmf/refinery/current/static_data/mediawiki/grouped_wikis/grouped_wikis_test.csv" \
#     --tables user_groups,ipblocks \
#     --from-timestamp YYYYMMDD000000 \
#     --to-timestamp YYYYMMDD000000 \
#     --partition-name snapshot \
#     --partition-value 2018_02_20_test \
#     --log-file sqoop-info-and-above.log

"""
Sqoops a list of tables from a list of wikis into a target HDFS location
NOTE: the options in the Usage below are in weird order because of a bug
in docopt that makes it fail if -- is the first thing on a line, we should
upstream a fix for that.

Usage:
  sqoop-mediawiki-tables --jdbc-host HOST --output-dir HDFS_PATH
          [--labsdb] --user NAME --password-file FILE
          [--max-tries TRIES] --wiki-file WIKIS --tables TABLES
          [--verbose] --from-timestamp FROM --to-timestamp TO
          [--force] --partition-name PART --partition-value PVAL
          [--mappers NUM] [--processors NUM] [--tmp-base-path TMP]
          [--job-name JOB_NAME] [--jar-file JAR_IN] --output-format FMT
          [--log-file LOG] [--dry-run]

Options:
    -h --help                           Show this help message and exit.
    -v --verbose                        Turn on verbose debug logging.

    -H HOST --jdbc-host HOST            Domain name of the mysql db
    -d HDFS_PATH --output-dir HDFS_PATH Target hdfs directory to write to

    -w FILE --wiki-file FILE            File with list of wiki dbs to sqoop
                                          A csv file of the form:

                                          dbname,parallel-group,...

                                          where all wiki dbs that will be
                                          sqooped in parallel with this one
                                          share the same parallel-group
    -t TABLES --tables TABLES           Comma-separated list of mediawiki table names.
                                        Limit the import to these specific tables.  This
                                        is required and jobs using this script should
                                        coordinate so as to not import the same table
                                        in two different ways.
    -F FROM --from-timestamp FROM       Get records with time >= this for all tables that
                                        have timestamps.  Format is YYYYMMDD000000
    -T TO --to-timestamp TO             Get records with time <  this for all tables that
                                        have timestamps.  Format is YYYYMMDD000000
    -s PART --partition-name PART       The name of the partition to use for this data.
                                        (for example, in month=2018-02, --partition-name
                                        is snapshot)
    -x PVAL --partition-value PVAL      The value to write to --partition-name for data
                                        imported with this script, (for example, in
                                        snapshot=2018-02, --partition-value is 2018-02)
    -u NAME --user NAME                 mysql user to use
    -p FILE --password-file FILE        File with mysql password to use

    -m NUM --mappers NUM                The number of mappers to use to sqoop big tables
                                        If the group of wikis from --wiki-file is larger
                                        than 3 wikis, this number will get changed to 1
                                        regardless of what is passed in, to prevent too
                                        many parallel connections to the source database
                                        [optional] default is 1
    -k NUM --processors NUM             The number of parallel processors sqooping
                                        If the group of wikis from --wiki-file is smaller
                                        than 3 wikis, this number will be changed to 1
                                        regardless of what is passed in, to prevent too
                                        many parallel sqoop jobs on a big wiki
                                        [optional] default is the number of
                                        processors on the machine
    -j JOB_NAME --job-name JOB_NAME     The yarn job name prefix, only one job with
                                        a certain prefix can run at a time.
                                        [optional] default is sqoop-mediawiki-tables
    -e TRIES --max-tries TRIES          Maximum number of tries for a sqoop job in
                                        case of failure [default: 3]
    -a FMT --output-format FMT          Output format for the sqoop jobs. Accepted
                                        values are 'avrodata' or 'parquet'.
    -b TMP --tmp-base-path TMP          HDFS folder where to temporary sqoop data
                                        before renaming. [default: /tmp/sqoop-mediawiki-tables]
    -l --labsdb                         Add '_p' postfix to table names for labsdb
    -f --force                          Deleting existing folders before importing
                                        instead of failing
    -r JAR_IN --jar-file JAR_IN         Disable code generation and use a jar file
                                        with pre-compiled ORM classes.  The class names
                                        will be convention-based and assumed to be the
                                        same as running this script with -g
    -o LOG --log-file LOG               A log file to send all output to.  If not passed,
                                        the script will only log WARN to console.
    -y --dry-run                        The script will run and log messages, but it
                                        won't load any data or have any consequences.
"""
__author__ = 'Dan Andreesu <milimetric@wikimedia.org>'

import csv
import logging
import os
import sys
import subprocess

from docopt import docopt
from concurrent import futures
from itertools import groupby
from traceback import format_exc
from tempfile import mkstemp

from refinery.logging_setup import configure_logging
from refinery.sqoop import (
    check_hdfs_path_or_exit, check_already_running_or_exit,
    SqoopConfig, sqoop_wiki, validate_tables_and_get_queries,
)

logger = logging.getLogger()


if __name__ == '__main__':
    # parse arguments
    arguments = docopt(__doc__)
    verbose                             = arguments.get('--verbose')
    yarn_job_name_prefix                = arguments.get('--job-name')

    target_hdfs_directory               = arguments.get('--output-dir')
    host                                = arguments.get('--jdbc-host')
    user                                = arguments.get('--user')
    password_file                       = arguments.get('--password-file')

    db_list_file                        = arguments.get('--wiki-file')
    import_tables_str                   = arguments.get('--tables')
    labsdb                              = arguments.get('--labsdb')

    from_timestamp                      = arguments.get('--from-timestamp')
    to_timestamp                        = arguments.get('--to-timestamp')
    partition_name                      = arguments.get('--partition-name')
    partition_value                     = arguments.get('--partition-value')

    num_mappers                         = int(arguments.get('--mappers') or '1')
    num_processors                      = int(arguments.get('--processors')) if arguments.get('--processors') else None
    output_format                       = arguments.get('--output-format')
    tmp_base_path                       = arguments.get('--tmp-base-path')
    max_tries                           = int(arguments.get('--max-tries'))
    dry_run                             = arguments.get('--dry-run')
    force                               = arguments.get('--force')
    jar_file                            = arguments.get('--jar-file')
    log_file                            = arguments.get('--log-file')

    level = logging.DEBUG if verbose else logging.INFO
    configure_logging(logger, level, log_file=log_file)

    logger.info('************ NOTE ************')
    logger.info('When sqooping from labs, resulting data will be shareable with the public '
                'but when sqooping from production, resulting data may need to be redacted or '
                'otherwise anonymized before sharing.')
    logger.info('^^^^^^^^^^^^ NOTE ^^^^^^^^^^^^')

    if dry_run:
        logger.info('**********************************************')
        logger.info('**************** DRY RUN MODE ****************')
        logger.info('**** Only logging, no action in this mode ****')
        logger.info('**********************************************')

    if output_format not in ['avrodata', 'parquet']:
        raise Exception('Invalid output-format {}'.format(output_format))

    logger.debug('Started Shell with with {}'.format(' '.join(arguments)))

    table_path_template = '{hdfs}/{table}/{partition_name}={partition_value}'.format(
        hdfs=target_hdfs_directory,
        table='{table}',
        partition_name=partition_name,
        partition_value=partition_value,
    )
    import_tables = import_tables_str.split(',')

    yarn_job_name_prefix = yarn_job_name_prefix or 'sqoop-mediawiki-tables'
    queries = validate_tables_and_get_queries(
        import_tables, from_timestamp, to_timestamp, labsdb
    )

    check_already_running_or_exit(yarn_job_name_prefix)
    logger.debug('No other running instances, proceeding')

    check_hdfs_path_or_exit(queries.keys(), table_path_template, tmp_base_path, force, dry_run)
    logger.debug('All table paths are empty or force cleaned')

    jdbc_host = 'jdbc:mysql://' + host + '/'
    dbpostfix = '_p' if labsdb else ''

    # read in the wikis to process and sqoop each one
    with open(db_list_file) as dbs_file:
        # Remove lines starting with hashes
        flat_wikis = [row for row in csv.reader(dbs_file) if not row[0].startswith('#')]

    failed_jobs = []
    for group, wikis in groupby(flat_wikis, lambda w: w[1]):
        logger.debug('Processing group {}'.format(group))
        executor_config_list = []
        wikiList = list(wikis)

        # If there are many wikis in a group set num_mappers to 1
        # This is because you don't want to sqoop in parallel when the amount of data is small
        # Also, do the opposite with num_processors for the same reason
        num_mappers_adjusted = num_mappers
        num_processors_adjusted = 1
        if len(wikiList) > 3:
            num_mappers_adjusted = 1
            num_processors_adjusted = num_processors

        for w in wikiList:
            for table in queries.keys():
                query = queries[table].get('query')
                map_types = queries[table]['map-types'] if ('map-types' in queries[table]) else None
                split_by = queries[table]['split-by']
                executor_config_list.append(SqoopConfig(yarn_job_name_prefix, user, password_file, jdbc_host,
                                            num_mappers_adjusted, output_format, tmp_base_path,
                                            table_path_template, w[0], dbpostfix, table,
                                            query, split_by, map_types,
                                            None, jar_file, 1, dry_run))

        # sqoop all wikis in this group and wait for them all to finish with retry
        with futures.ProcessPoolExecutor(num_processors_adjusted) as executor:
            current_try = 0
            while (executor_config_list and current_try < max_tries):
                executor_config_list = filter(None, list(executor.map(sqoop_wiki, executor_config_list)))
                current_try += 1
        failed_jobs.extend(executor_config_list)

    # Write success flags for tables NOT having failed-job
    # Get tables having at least one failed job
    failed_tables = set(map(lambda config: config.table, failed_jobs))
    for table in (set(queries.keys()) - failed_tables):
        success_file_path = table_path_template.format(table=table) + '/_SUCCESS'
        if not dry_run:
            subprocess.check_call([
                'hdfs', 'dfs', '-touchz',
                success_file_path
            ])
        logger.info('Wrote Success file {}'.format(success_file_path))

    # In any mode, if failed jobs, send an error email and log error
    if failed_jobs:
        logger.error('*' * 50)
        logger.error('*  Jobs to re-run:')
        for c in failed_jobs:
            logger.error('*    - {}'.format(str(c)))
        logger.error('*' * 50)

        sys.exit(1)
