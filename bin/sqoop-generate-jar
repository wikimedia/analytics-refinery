#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Example run:
#   python3 sqoop-generate-jar \
#       --output-dir /home/milimetric/sqoop-jar \
#       --user research \
#       --password-file /user/milimetric/mysql-analytics-research-client-pw.txt \
#       --verbose

"""
Generate java classes for each table we know how to sqoop, then bundle them
into a JAR for later use. Save the generated jar to JAR_OUT, in a file named
JAR_OUT/mediawiki-tables-sqoop-orm.jar
NOTE: the options in the Usage below are in weird order because of a bug
in docopt that makes it fail if -- is the first thing on a line, we should
upstream a fix for that.

Usage:
  sqoop-generate-jar --output-dir JAR_OUT
          [--clouddb] --user NAME --password-file FILE
          [--verbose] [--job-name JOB_NAME]

Options:
    -h --help                           Show this help message and exit.
    -v --verbose                        Turn on verbose debug logging.

    -d JAR_OUT --output-dir JAR_OUT     Target directory to write the jar to

    -u NAME --user NAME                 mysql user to use
    -p FILE --password-file FILE        File with mysql password to use

    -j JOB_NAME --job-name JOB_NAME     The yarn job name prefix, only one job with
                                        a certain prefix can run at a time.
                                        [optional] default is sqoop-generate-jar
    -l --clouddb                        Use the cloud cluster with different dbnames
                                        and potentially different queries
"""
__author__ = 'Dan Andreesu <milimetric@wikimedia.org>'

import csv
import os
import sys
import logging
import subprocess

from docopt import docopt
from concurrent import futures
from itertools import groupby
from traceback import format_exc
from tempfile import mkstemp

from refinery.logging_setup import configure_logging
from refinery.sqoop import (
    check_already_running_or_exit,
    SqoopConfig, sqoop_wiki, validate_tables_and_get_queries,
)
from refinery.util import get_mediawiki_section_dbname_mapping, get_jdbc_string

logger = logging.getLogger()


if __name__ == '__main__':
    # parse arguments
    arguments = docopt(__doc__)
    verbose                             = arguments.get('--verbose')
    clouddb                             = arguments.get('--clouddb')
    yarn_job_name_prefix                = arguments.get('--job-name')

    target_jar_directory                = arguments.get('--output-dir')
    user                                = arguments.get('--user')
    password_file                       = arguments.get('--password-file')

    db_mapping                          = get_mediawiki_section_dbname_mapping()

    level = logging.DEBUG if verbose else logging.INFO
    configure_logging(logger, level)

    yarn_job_name_prefix = yarn_job_name_prefix or 'sqoop-generate-jar'
    check_already_running_or_exit(yarn_job_name_prefix)

    logger.info('Started Shell with with {}'.format(' '.join(arguments)))

    queries = validate_tables_and_get_queries(None, None, None)
    jar_path = os.path.join(target_jar_directory, 'mediawiki-tables-sqoop-orm.jar')

    failed_jobs = []
    executor_config_list = []

    # NOTE: using etwiki to generate schema for all wikis,
    # but it might not always be in sync with the other dbs
    example_dbname = 'etwiki'
    jdbc_string = get_jdbc_string(example_dbname, clouddb, db_mapping)
    for table in queries.keys():
        executor_config_list.append(
            SqoopConfig(yarn_job_name_prefix, user, password_file, jdbc_string, 1,
                        None, None, None, None, example_dbname, table, queries,
                        target_jar_directory, None, 1, False)
        )

    with futures.ProcessPoolExecutor(1) as executor:
        failed_jobs.extend(filter(None, list(executor.map(sqoop_wiki, executor_config_list))))
        if len(failed_jobs):
            failed_tables = ', '.join([c.table for c in failed_jobs])
            logger.error('ERROR generating ORM jar for {}'.format(failed_tables))
            sys.exit(1)

    subprocess.check_call([
        'jar', 'cf', jar_path, '-C', target_jar_directory, '.'
    ])
    logger.info('Generated ORM jar at {}'.format(jar_path))
