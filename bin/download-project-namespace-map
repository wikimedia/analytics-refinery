#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Note: You should make sure to put refinery/python on your PYTHONPATH.
#   export PYTHONPATH=$PYTHONPATH:/path/to/refinery/python
#
# Will be scheduled on a cron, every 7 days, as:
#   download-project-namespace-map -c \
#     --output-hdfs /wmf/data/raw/mediawiki/project_namespace_map

"""
Writes the WMF site matrix as projects with namespace information to a file

Usage:
  download-project-namespace-map.py (--output-file FILE|--output-hdfs PATH)
          [-c] [-v] [-s SNAPSHOT] [-p PROXY]

Options:
    -h --help                           Show this help message and exit.
    -v --verbose                        Turn on verbose debug logging.
    -c --include-closed                 Include wikis that are now closed.
    -o FILE --output-file FILE          Output the results here
                                        (default: ./project-namespace-map.csv).
    -x PATH --output-hdfs PATH          Output the results to HDFS
    -s SNAPSHOT --snapshot SNAPSHOT     The snapshot partition to load data
                                        into (usually YYYY-MM) when writing
                                        to HDFS
    -p PROXY --proxy PROXY              The HTTPS proxy to use
"""
__author__ = 'Dan Andreesu <milimetric@wikimedia.org>'

import requests
import json
import csv
import logging
import os
import sys
import pymysql

from docopt import docopt
from tempfile import mkstemp
from subprocess import check_call, run, PIPE
from refinery.util import  get_mediawiki_section_dbname_mapping, get_dbstore_host_port

outfile = 'project_namespace_map.csv'

headers = {
    'User-Agent': 'Wikimedia Foundation Analytics Bot',
    'From': 'dandreescu@wikimedia.org'
}

wikis_query = ''.join([
    'https://www.mediawiki.org/w/api.php?action=sitematrix',
    '&smsiteprop=code|dbname|sitename|lang|url',
    '&smstate=all',
    '&format=json',
])

namespace_query = ''.join([
    '/w/api.php?action=query',
    '&format=json',
    '&meta=siteinfo',
    '&siprop=general|namespaces',
])

analytics_option_file = '/user/analytics/mysql-analytics-research-client-pw.txt'
cloud_option_file = '/user/analytics/mysql-analytics-labsdb-client-pw.txt'
analytics_user = 'research'
cloud_user = 's53272'

if __name__ == '__main__':
    # parse arguments
    arguments = docopt(__doc__)
    verbose = arguments['--verbose']
    outfile = arguments['--output-file']
    outhdfs = arguments['--output-hdfs']
    closed = arguments['--include-closed']
    snapshot = arguments['--snapshot']
    proxy = arguments['--proxy']

    # If we're outputting to hdfs, output to a temp file and copy up
    output_to_hdfs = outhdfs is not None
    if output_to_hdfs:
        outfile = mkstemp()[1]

    log_level = logging.INFO
    if verbose:
        log_level = logging.DEBUG

    proxies = {'https': proxy} if proxy else {}

    logging.basicConfig(level=log_level,
                        format='%(asctime)s %(levelname)-6s %(message)s',
                        datefmt='%Y-%m-%dT%H:%M:%S')


def get_wikis(closed):
    """
    Retrieves a list of Wikimedia wikis from the sitematrix API.

    Args:
        closed (bool): Whether to include closed wikis.

    Returns:
        list[dict]: A list of wiki metadata dictionaries.
    """
    matrix = requests.get(
        wikis_query,
        headers=headers,
        proxies=proxies
    ).json().get('sitematrix', {})

    wikis = [

        wiki

        for language in matrix.values()
        if type(language) is dict and 'site' in language

        for wiki in language['site']
    ] + [

        wiki

        for wiki in matrix.get('specials', [])
    ]

    return [
        wiki
        for wiki in wikis
        if 'private' not in wiki and (closed or 'closed' not in wiki)
    ]


def get_password(path, cache, key):
    """
    Retrieve and cache Databases password from HDFS.

    Args:
        path (str): HDFS path to password file.
        cache (dict): Shared cache dict. Db_replica_id -> password
        key (str): Cache key.

    Returns:
        str: Password.
    """
    if key not in cache:
        logging.debug(f"Loading password from {path}")
        cmd = f'hdfs dfs -cat {path}'
        result = run(cmd, shell=True, stdout=PIPE, stderr=PIPE, universal_newlines=True)
        if result.returncode != 0:
            raise RuntimeError(f"Failed to read password from {path}: {result.stderr}")
        cache[key] = result.stdout.strip()
    return cache[key]

def db_has_tables(hostname, port, user, password, db):
    """
    Check if the given wiki_db has tables in database replica.
    If no tables then wiki_db doesn't exist in the replica.

    Args:
        hostname (str): DB host.
        port (int): DB port.
        user (str): DB user.
        password (str): DB password.
        db (str): Database name.

    Returns:
        bool: True if tables exist, False otherwise.
    """
    try:
        with pymysql.connect(
                host=hostname,
                port=int(port),
                user=user,
                password=password,
                db=db,
                connect_timeout=5
        ) as conn:
            with conn.cursor() as cur:
                cur.execute("SHOW TABLES")
                return bool(cur.fetchall())
    except Exception as e:
        logging.warning(f"DB {db} does not exists on {hostname}: {e}")
        return False

def check_cloud(db, db_mapping, passwd_cache={}):
    """
    Checks whether a given database exists in both analytics and cloud replicas.

    Args:
        db (str): The database name (e.g. 'enwiki').
        db_mapping (dict): Mapping of dbname keys to
            mediawiki database section values
        passwd_cache (dict): Cache for replicas passwords.

    Returns:
       bool: True if the DB exists in both analytics and cloud replicas, False otherwise.
   """

    analytics_host, analytics_port = get_dbstore_host_port(False, db, False, db_mapping)
    cloud_host, cloud_port = get_dbstore_host_port(False, db, True, db_mapping)

    analytics_passwd = get_password(analytics_option_file, passwd_cache, 'analytics')
    cloud_passwd = get_password(cloud_option_file, passwd_cache, 'cloud')

    return (
            db_has_tables(analytics_host, analytics_port, analytics_user, analytics_passwd, db)
            and db_has_tables(cloud_host, cloud_port, cloud_user, cloud_passwd, f"{db}_p")
    )

logging.info('Fetching wiki database to section value mappings')

wikidb_mapping=get_mediawiki_section_dbname_mapping()

wikis = get_wikis(closed)
namespaceDictionary = {}

logging.info('Fetching namespace info for {} wikis'.format(len(wikis)))

###
# Writes mapping as: (hostname, dbname, ns integer, ns canonical, ns localized, content, is closed, has cloud replica)
# hostname      : ja.wikipedia.org
# language      : ja
# sitename      : Wikipedia
# dbname        : jawiki
# home page     : https://ja.wikipedia.org/wiki/%E3%83%A1%E3%82%A4%E3%83%B3%E3%83%9A%E3%83%BC%E3%82%B8
# mw version    : MediaWiki 1.41.0-wmf.28
# case setting  : first-letter
# namespace     : 2, 100, etc.
# ns canonical  : the english prefix if exists, otherwise the localized prefix
# ns localized  : the localized prefix
# content       : whether or not this namespace is a content namespace
# is closed     : whether or not the wiki of this dbname is close down.
# has cloud replica : whether or not this dbname exists in cloud replica (i.e should be ingested)
###

wiki_mappings_written = 0
passwd_cache = {}

with open(outfile, 'w', encoding='utf-8') as w:
    spamwriter = csv.writer(w)

    for wiki in wikis:
        site = wiki.get('url', '')
        sitename = wiki.get('sitename', '')
        lang = wiki.get('lang', '')
        host = site.replace('https://', '')
        dbname = wiki.get('dbname', host)
        is_closed = 'closed' in wiki
        try:
            r = requests.get(site + namespace_query, headers=headers, proxies=proxies)
            query = json.loads(r.text)['query']
            general = query['general']
            home_page = general.get('base', host)
            mw_version = general.get('generator', 'Unknown MediaWiki Version')
            case_setting = general.get('case', 'uknown case')
            ns = query['namespaces']
            has_cloud_replica = check_cloud(dbname, wikidb_mapping)

            for ns_key, v in ns.items():
                is_content = 0
                if 'content' in v:
                    is_content = 1

                row = [
                    host,
                    lang,
                    sitename,
                    dbname,
                    home_page,
                    mw_version,
                    case_setting,
                    ns_key,
                    v.get('canonical', ''),
                    v.get('*', ''),
                    v.get('case', 'unknown case'),
                    is_content,
                    is_closed,
                    has_cloud_replica,
                ]
                spamwriter.writerow(row)
                wiki_mappings_written += 1

        except Exception:
            logging.exception(site + ' FAILED!!!')


if wiki_mappings_written == 0:
    logging.error('The number of wiki mappings written to the CSV file is zero.')
    sys.exit(1)

if output_to_hdfs:
    if (snapshot is not None):
        outhdfs += '/snapshot=' + snapshot

    check_call([
        'hdfs', 'dfs', '-mkdir', '-p',
        outhdfs,
    ])

    check_call([
        'hdfs', 'dfs', '-put', '-f',
        outfile,
        outhdfs + '/project_namespace_map',
    ])
    check_call([
        'hdfs', 'dfs', '-touchz',
        outhdfs + '/_SUCCESS',
    ])
    # clean up the temp file
    os.remove(outfile)
