#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Note: You should make sure to put refinery/python on your PYTHONPATH.
#   export PYTHONPATH=$PYTHONPATH:/path/to/refinery/python

"""
Automatically drops old Hive partitions from Hive tables
and deletes the hourly time bucketed directories from HDFS.

NOTE: This script should replace refinery-drop-hourly-partitions once
all uses are ported over to it.

Usage: refinery-drop-hive-partitions [options]

Options:
    -h --help                           Show this help message and exit.
    -d --older-than-days=<days>         Drop data older than this number of days.  [default: 60]
    -D --database=<dbname>              Hive database name.  [default: default]
    -t --tables=<tables>                Comma separated list of tables to check.  Defaults to
                                        all tables in database.
    -l --limit=<limit>                  Only drop from this many tables.  Useful for testing.
    -N --partition-depth=<n>            Number of directories to use when constructing partition file glob.
                                        If not given, this will be infered from the first partition
                                        found in the table.  If the table has noÂ current partitions,
                                        this will throw an exeption.
    -o --hive-options=<options>         Any valid Hive CLI options you want to pass to Hive commands.
                                        Example: '--auxpath /path/to/hive-serdes-1.0-SNAPSHOT.jar'
    -v --verbose                        Turn on verbose debug logging.
    -f [FILE] --log-file [FILE]         File to send info logs to
    -n --dry-run                        Don't actually drop any partitions, just output the Hive queries to drop partitions.
"""
__author__ = 'Andrew Otto <otto@wikimedia.org>'

import datetime
from docopt import docopt
import logging
import re
import os
import sys
from refinery.hive import Hive, HivePartition
from refinery.hdfs import Hdfs
from refinery.logging_setup import configure_logging

logger = logging.getLogger()

if __name__ == '__main__':
    # parse arguments
    arguments = docopt(__doc__)

    days            = int(arguments['--older-than-days'])
    database        = arguments['--database']
    tables          = arguments['--tables']
    limit           = arguments['--limit']
    partition_depth = arguments['--partition-depth']
    hive_options    = arguments['--hive-options']
    verbose         = arguments['--verbose']
    log_file        = arguments['--log-file']
    dry_run         = arguments['--dry-run']


    log_level = logging.DEBUG if verbose else logging.INFO
    configure_logging(logger, log_level, log_file=log_file)

    if partition_depth is not None:
        partition_depth = int(partition_depth)

    # Delete partitions older than this.
    old_partition_datetime_threshold = datetime.datetime.now() - datetime.timedelta(days=days)

    # Instantiate Hive.
    hive = Hive(database, hive_options)

    if tables is not None:
        tables = tables.split(',')
    else:
        tables = hive.get_tables()

    if limit is not None:
        tables = tables[:int(limit)]

    # Iterate through each table and find old Hive partitions and HDFS paths to drop.
    for table in tables:
        logger.info('Looking for partitions to drop for {}.{}...'.format(database, table))
        # Attempt to infer table location from the table metadata.
        table_location = hive.table_location(table)

        partitions_to_drop        = []
        partition_paths_to_delete = []

        # Loop through all partitions for this table and drop anything that is too old.
        hive_partitions = hive.partitions(table)
        for partition in hive_partitions:
            if partition.datetime() < old_partition_datetime_threshold:
                partitions_to_drop.append(partition)

        # Managed tables have their files deleted by hive directly from the
        # drop table statement. Other tables need manual cleanup.
        table_type = hive.table_metadata(table)['Table Type']
        if table_type != 'MANAGED_TABLE':
            # Build a glob based on number of partition keys found in Hive partitions.
            # This will be used to find possible directories that need to be removed
            # from HDFS. TODO: This has a bug in that no data will be removed from HDFS
            # If not partitions were in the Hive table.
            if partition_depth is not None:
                partition_glob = os.path.join(*([table_location] + ['*'] * partition_depth))
            elif len(hive_partitions) > 0:
                partition_glob = hive_partitions[0].glob(base_path=table_location)
            else:
                partition_glob = None
                logger.warn(
                    'Could not search for HDFS paths to drop. '
                    'Could not construct partition glob from a partition depth.'
                )

            if partition_glob is not None:
                # Loop through all the partition directory paths for this table
                # and check if any of them are old enough for deletion.
                for partition_path in Hdfs.ls(partition_glob, include_children=False):
                    try:
                        partition = HivePartition(partition_path)
                        if partition.datetime() < old_partition_datetime_threshold:
                            partition_paths_to_delete.append(partition_path)
                    except Exception as e:
                        logger.error(
                            'Could not parse date from {}. Skipping. ({})'
                            .format(partition_path, e)
                        )
                        continue


        # Drop any old Hive partitions
        if partitions_to_drop:
            partition_specs_to_drop = [p.spec() for p in partitions_to_drop]
            if dry_run:
                print(hive.drop_partitions_ddl(table, partition_specs_to_drop))
            else:
                logger.info('Dropping {0} Hive partitions from table {1}.{2}'
                    .format(len(partition_specs_to_drop), database, table)
                )
                hive.drop_partitions(table, partition_specs_to_drop)
        else:
            logger.info('No Hive partitions need dropped for table {0}.{1}'.format(database, table))

        # Delete any old HDFS data
        if partition_paths_to_delete:
            if dry_run:
                print('hdfs dfs -rm -R -skipTrash ' + ' '.join(partition_paths_to_delete))
                print('Parent directories that have been emptied because of this operation will also be deleted.')

            else:
                logger.info('Removing {0} partition directories for table {1}.{2} from {3}.'
                    .format(len(partition_paths_to_delete), database, table, table_location)
                )
                Hdfs.rm(' '.join(partition_paths_to_delete))
                # Delete any directories left empty by partition removal
                # Keep track of directories that were checked for size
                MAX_NUMBER_OF_PARENT_DIRS_TO_REMOVE = 3

                visited_paths = set()
                parent_dirs_to_remove = []
                # Generate a list of parent folders per directory
                for path in partition_paths_to_delete:
                    parent_paths = Hdfs.get_parent_dirs(path, table_location)
                    # Sort them by length desc so we make sure we don't rmdir nonempty directories
                    parent_paths.sort(key=len)
                    parent_paths.reverse()
                    # Iterate over parent list. If we remove a dir, we jump to the next path
                    for parent_path in parent_paths:
                        if parent_path not in visited_paths:
                            visited_paths.add(parent_path)
                            if Hdfs.dir_bytes_size(parent_path) == 0:
                                parent_dirs_to_remove.append(parent_path)
                                break
                logger.info('Removing {0} empty directories for table {1}.{2}.'
                    .format(len(parent_dirs_to_remove), database, table))
                if len(parent_dirs_to_remove) > 0:
                    if len(parent_dirs_to_remove) > MAX_NUMBER_OF_PARENT_DIRS_TO_REMOVE:
                        errorString = 'Number of parent partition directories to remove ({0}) exceeds maximum allowed ({1})'.format(len(parent_dirs_to_remove), MAX_NUMBER_OF_PARENT_DIRS_TO_REMOVE)
                        logger.error(errorString)
                        sys.exit(errorString)
                    Hdfs.rmdir(' '.join(parent_dirs_to_remove))
        else:
            logger.info('No partition directories need removed for table {0}.{1}'.format(database, table))
