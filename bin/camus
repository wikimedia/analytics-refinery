#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""" Wrapper for launching Camus Kafka Importer and CamusPartitionChecker

Usage: camus [options] <properties-file>

Options:
    -h --help                           Show this help message and exit.
    -n --dry-run                        Prints command that would have been run without running anything.
    -r --run                            If set, a camus job will be launched using the given configuration
    -N --job-name=<job-name>            Hadoop JobName. [default: Camus Job]
    -j --jar=<jar-file>                 Path to Camus .jar file.  [default: /srv/deployment/analytics/refinery/artifacts/camus-wmf.jar]
    -l --libjars=<jar-file(s)>          Comma separated paths to external jars to make available to Camus' Map Reduce jobs
    -f --force                          If set, job will be submitted without checking if it is already running.
    -c --check                          If set, a CamusPartitionChecker job will be submitted after the camus run if any (checking and flagging imported partitions).
    -q --check-jar=<jar-file>           Path to refinery-job jar file. [default: /srv/deployment/analytics/refinery/artifacts/refinery-camus.jar]
    -d --check-datetime=<datetime>      Camus run date to check (yyyy-mm-dd-HH-MM-SS format). Most recent run if not set.
    -O --check-java-opts=<java-opts>    Any extra Java options to pass to the CamusPartitionChecker job.
    -E --check-emails-to=<check_emails_to>  If set, encountered errors will be emailed to these comma separated email addresses.
    -m --check-dry-run                  Execute the check in dry-run mode.
"""
__author__ = 'Andrew Otto <otto@wikimedia.org>'

import os
import sys
import logging
from docopt import docopt
from refinery.util import is_yarn_application_running



logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(levelname)-6s %(message)s',
                    datefmt='%Y-%m-%dT%H:%M:%S')

if __name__ == '__main__':
    # parse arguments
    arguments = docopt(__doc__)
    exit_value = 0

    properties_file = arguments['<properties-file>']
    dry_run = arguments['--dry-run']

    #Camus execution block
    if (arguments['--run']):
        jar             = arguments['--jar']
        job_name        = arguments['--job-name']
        libjars         = arguments['--libjars']

        if not arguments['--force'] and is_yarn_application_running(job_name):
            logging.warn('Not submitting camus job "{0}", it is currently running.'.format(job_name))
            sys.exit(1)

        libjars_opt = '' if not libjars else ('-libjars ' + libjars)
        camus_command = '/usr/bin/hadoop jar {0} com.linkedin.camus.etl.kafka.CamusJob {1} -P {2} -Dcamus.job.name="{3}"'.format(
            jar,
            libjars_opt,
            properties_file,
            job_name
        )
        logging.info('Submitting camus job "{0}": {1}'.format(job_name, camus_command))

        camus_res = 0
        if not dry_run:
            exit_value += os.system(camus_command)

    # Check execution block
    if (arguments['--check']):
        check_jar       = arguments['--check-jar']
        # We use Spark 2 jars/* to get Scala and Hadoop dependencies.
        check_classpath = ':'.join([check_jar, '/usr/lib/spark2/jars/*'])
        check_java_opts = arguments['--check-java-opts'] if arguments['--check-java-opts'] else ''
        datetime_to_check = '-d %s' % arguments['--check-datetime'] if arguments['--check-datetime'] else ''
        check_dry_run = '--dry-run' if arguments['--check-dry-run'] else ''
        check_email_report = '--send-email-report --to-emails %s' % arguments['--check-emails-to'] if arguments['--check-emails-to'] else ''

        # the /etc/hadoop/conf bit is needed to allow parameters to be picked
        # up to allow things like kerberos auth to work properly.
        # T226232
        checker_command = '/usr/bin/java -cp "{0}:/etc/hadoop/conf" {1} org.wikimedia.analytics.refinery.camus.CamusPartitionChecker -c {2} {3} {4} {5}'.format(
            check_classpath,
            check_java_opts,
            properties_file,
            datetime_to_check,
            check_dry_run,
            check_email_report
        )

        logging.info('Submitting camus-checker job: {0}'.format(checker_command))
        if not dry_run:
            exit_value += os.system(checker_command)

    sys.exit(exit_value)
