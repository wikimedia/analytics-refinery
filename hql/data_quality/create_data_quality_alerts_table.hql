-- Create table statement for the data quality alerts table.
--
-- Usage:
--     sudo -u analytics spark3-sql -f create_data_quality_alerts_table.hql \
--     --database wmf_data_ops \
--     -d location=/wmf/data/data_quality/alerts/
--
CREATE EXTERNAL TABLE `data_quality_alerts` (
    source_table        STRING          COMMENT 'The table metrics are computed on.',
    partition_id        STRING          COMMENT 'Identifier of the partition of table the metrics are computed on. e.g. year=2024/month=1/day=1.',
    partition_ts        TIMESTAMP       COMMENT 'A Timestamp representation of partition_id.',
    status              STRING          COMMENT 'AWS Deequ validation suite: constraint check status. e.g. one of Success or Failure.',
    severity_level      STRING          COMMENT 'AWS Deequ validation suite: constraint check severity level.e.g. one of Error or Warning.',
    value               DOUBLE          COMMENT 'AWS Deequ validation suite: constraint value.',
    constraint          STRING          COMMENT 'AWS Deequ validation suite: identifier of this constraint type.e.g. MaximumConstraint(...).',
    error_message       STRING          COMMENT 'AWS Deequ validation suite: error message generated by deequ.',
    pipeline_run_id     STRING          COMMENT 'A unique identifier of the orchestrator that generated the metric.e.g. this could an Airflow run_id.'
)
USING iceberg TBLPROPERTIES ('format-version'='2')
PARTITIONED BY (
    years(`partition_ts`),
    `source_table`          -- source_table partitioning helps concurrent writers not step on each other
)
LOCATION '${location}'
;