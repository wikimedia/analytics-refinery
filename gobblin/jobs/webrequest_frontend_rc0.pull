#
# Pulls specific webrequest streams discovered via EventStreamConfig
#

include=/srv/deployment/analytics/refinery/gobblin/common/kafka_to_hdfs_hourly.properties

job.name=webrequest_frontend_rc0
job.group=gobblin
extract.namespace=org.wikimedia.analytics.webrequest_frontend

event_stream_config.uri=https://meta.wikimedia.org/w/api.php
# Use WMF production network settings (this allows us to access meta.wikimedia.org without an http proxy).
event_stream_config.is_wmf_production=true
# Filter for streams that match these settings.
# These setting are declared in EventStreamConfig extension.
event_stream_config.settings_filters=/consumers/analytics_hadoop_ingestion/job_name:webrequest_frontend,/consumers/analytics_hadoop_ingestion/enabled:true

# dt is the preferred timestamp for this dataset.
# See the project Decision Record doc for details.
# Ref: https://phabricator.wikimedia.org/T354694
writer.partition.timestamp.columns=dt

# We want to have one mapper per kafka partition.
# If we have less partitions than mappers, the reminder should
# stay idle.
# TODO: tweak based on how webrequest_text and webrequest_upload will
# be partitioned.
mr.job.max.mappers=48

bootstrap.with.offset=latest

# TODO: currently the webrequest_frontent job pulls a rc0 version for the stream.
# Publish to webrequest_frontent once the stream is GA.
data.publisher.final.dir=/wmf/data/raw/webrequest_frontend_rc0
