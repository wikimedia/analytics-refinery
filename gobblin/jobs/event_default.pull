#
# Pulls event streams discovered via stream config with job_name:event_default.
#

include=/srv/deployment/analytics/refinery/gobblin/common/kafka_to_hdfs_hourly.properties

job.name=event_default
job.group=gobblin
extract.namespace=org.wikimedia.analytics.event_default

event_stream_config.uri=https://meta.wikimedia.org/w/api.php
# Use WMF production network settings (this allows us to access meta.wikimedia.org without an http proxy).
event_stream_config.is_wmf_production=true
# Filter for streams that match these settings.
event_stream_config.settings_filters=/consumers/analytics_hadoop_ingestion/job_name:event_default,/consumers/analytics_hadoop_ingestion/enabled:true

writer.partition.timestamp.columns=meta.dt


# There are quite a few event streams topic partitions, and usually we want to have more mappers
# than number of partitions.  As of 2021-07 there are 200-300 event stream topic partitions.
# However, most of the partitions will be really small and will
# finish import very quickly. We don't yet know how many mappers can be doing real work
# in parallel, but set this to 128 now to try to find a balance between parallelizing
# and reusing mapper task processes if they finish quickly.
mr.job.max.mappers=128

bootstrap.with.offset=latest

data.publisher.final.dir=/wmf/data/raw/event
