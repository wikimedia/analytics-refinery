# Common properties for a kafka-to-hdfs_hourly_partitioned job
# Data is expected be either timestamped in kafka or json

# TODO: DRY this with Airflow and/or puppet.
kafka.brokers=kafka-jumbo1001.eqiad.wmnet:9093,kafka-jumbo1002.eqiad.wmnet:9093,kafka-jumbo1003.eqiad.wmnet:9093,kafka-jumbo1004.eqiad.wmnet:9093,kafka-jumbo1005.eqiad.wmnet:9093,kafka-jumbo1006.eqiad.wmnet:9093,kafka-jumbo1007.eqiad.wmnet:9093,kafka-jumbo1008.eqiad.wmnet:9093,kafka-jumbo1009.eqiad.wmnet:9093

source.class=org.wikimedia.gobblin.kafka.Kafka1TimestampedRecordSource
kafka.accepted.timestamps=CreateTime
gobblin.kafka.consumerClient.class=org.apache.gobblin.kafka.client.Kafka1ConsumerClient$Factory
source.kafka.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer

source.kafka.security.protocol=SSL
source.kafka.ssl.cipher.suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384

writer.builder.class=org.wikimedia.gobblin.writer.TimestampedStringRecordDataWriterBuilder
simple.writer.delimiter=\n
writer.file.path.type=tablename
writer.destination.type=HDFS
writer.output.format=txt
writer.codec.type=gz
writer.partition.timezone=UTC

writer.partitioner.class=org.wikimedia.gobblin.writer.partitioner.TimestampedRecordOrJsonStringTimeBasedWriterPartitioner
writer.partition.pattern=\'year=\'yyyy\'/month=\'M\'/day=\'d\'/hour=\'H

data.publisher.type=org.wikimedia.gobblin.publisher.TimePartitionedDataPublisherWithFlag

metrics.reporting.file.enabled=true
