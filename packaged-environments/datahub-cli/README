This directory contains the files necessary to manually build
a conda environment for the datahub ingestion CLI.  Instructions:

1. clone workflow_utils: https://gitlab.wikimedia.org/repos/data-engineering/workflow_utils
2. pip install -e .
3. install miniconda: https://docs.conda.io/en/latest/miniconda.html
4. make sure you have headers for sasl: https://pypi.org/project/sasl/0.1.3/
5. in this directory, run `conda dist`

If everything worked, you should have an archive file with the conda environment in ./dist
If something broke, ask on IRC in #wikimedia-analytics

At this point, go ahead and upload the archive to archiva so we can reference it from airflow jobs.  For this environment, we've used:

* Group ID: datahub
* Artifact ID: cli
* Version: eg 0.8.43
* Packaging: tgz
(leave Generate Maven POM unchecked)

Example: https://archiva.wikimedia.org/#artifact/datahub/cli/0.8.43

Archiva docs at: https://wikitech.wikimedia.org/wiki/Analytics/Systems/Archiva
