<?xml version="1.0" encoding="UTF-8"?>
<coordinator-app xmlns="uri:oozie:coordinator:0.4"
    xmlns:sla="uri:oozie:sla:0.2"
    name="mediacounts-archive-coord"
    frequency="${coord:days(1)}"
    start="${start_time}"
    end="${stop_time}"
    timezone="Universal">

    <parameters>

        <!-- Required properties. -->
        <property><name>queue_name</name></property>
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive2_jdbc_url</name></property>
        <property><name>start_time</name></property>
        <property><name>stop_time</name></property>
        <property><name>mediacounts_datasets_file</name></property>
        <property><name>mediacounts_data_directory</name></property>
        <property><name>hive_site_xml</name></property>
        <property><name>workflow_file</name></property>
        <property><name>mediacounts_table</name></property>
        <property><name>mark_directory_done_workflow_file</name></property>
        <property><name>temporary_directory</name></property>
        <property><name>mediacounts_daily_archive_directory</name></property>
        <property><name>archive_job_output_workflow_file</name></property>
        <property><name>send_error_email_workflow_file</name></property>
        <property><name>sla_alert_contact</name></property>
    </parameters>

    <controls>
        <!--
        By having materialized jobs not timeout, we ease backfilling incidents
        after recoverable hiccups on the dataset producers.
        -->
        <timeout>-1</timeout>

        <!--
        Since the job only runs daily, even low concurrency allows to catch up
        pretty fast. Hence, we can limit concurrency to 1, as the tsvs typically
        process quite some data.
        -->
        <concurrency>1</concurrency>

        <!--
        In order to keep backfilling after an incident simple, we only start
        throttling materialization after 4 days.
        Due to the low concurrency, and low discrepancy between progressing
        time, and expected availability of datasets, we should typically have
        far less materialized jobs.
        -->
        <throttle>4</throttle>
    </controls>

    <datasets>
        <include>${mediacounts_datasets_file}</include>
    </datasets>

    <input-events>
        <data-in name="mediacounts" dataset="mediacounts_hourly">
            <start-instance>${coord:current(0)}</start-instance>
            <end-instance>${coord:current(23)}</end-instance>
        </data-in>
    </input-events>

    <action>
        <workflow>
            <app-path>${workflow_file}</app-path>
            <configuration>
                <property>
                    <name>year</name>
                    <value>${coord:formatTime(coord:nominalTime(), "yyyy")}</value>
                </property>
                <property>
                    <name>month</name>
                    <value>${coord:formatTime(coord:nominalTime(), "MM")}</value>
                </property>
                <property>
                    <name>day</name>
                    <value>${coord:formatTime(coord:nominalTime(), "dd")}</value>
                </property>
            </configuration>
        </workflow>

        <sla:info>
            <!--
                Use action actual time as SLA base, since it's the time used
                to compute timeout
                We use 30 hours as one day of webrequest data needs to be
                present for the job to process and webrequest-sla is 5 hours
            -->
            <sla:nominal-time>${coord:actualTime()}</sla:nominal-time>
            <sla:should-end>${30 * HOURS}</sla:should-end>
            <sla:alert-events>end_miss</sla:alert-events>
            <sla:alert-contact>${sla_alert_contact}</sla:alert-contact>
        </sla:info>
    </action>
</coordinator-app>
