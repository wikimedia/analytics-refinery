# Configures a coordinator to generate a daily mediacounts report
#
# Usage:
#     oozie job -run \
#         -config oozie/mediacounts/archive/bundle.properties
#
# NOTE:  The $oozie_directory must be synced to HDFS so that all relevant
#        .xml files exist there when this job is submitted.


name_node                           = hdfs://analytics-hadoop
job_tracker                         = resourcemanager.analytics.eqiad.wmnet:8032
hive_principal                      = hive/analytics-hive.eqiad.wmnet@WIKIMEDIA
hive2_jdbc_url                      = jdbc:hive2://analytics-hive.eqiad.wmnet:10000/default
queue_name                          = default

user                                = analytics

# Base path in HDFS to refinery.
# When submitting this job for production, you should
# override this to point directly at a deployed
# directory name, and not the 'symbolic' 'current' directory.
# E.g.  /wmf/refinery/2015-01-05T17.59.18Z--7bb7f07
refinery_directory                  = ${name_node}/wmf/refinery/current

# Base path in HDFS to oozie files.
# Other files will be used relative to this path.
oozie_directory                     = ${refinery_directory}/oozie

# HDFS path to workflow to run.
workflow_file                       = ${oozie_directory}/mediacounts/archive/workflow.xml

# HDFS path to mediacounts dataset definition
mediacounts_datasets_file           = ${oozie_directory}/mediacounts/datasets.xml

# Initial import time of the mediacounts dataset.
# Make sure to have hours and minutes at 0!
start_time                          = 2014-04-01T00:00Z

# Time to stop running this coordinator.  Year 3000 == never!
stop_time                           = 3000-01-01T00:00Z

# HDFS path to workflow to mark a directory as done
mark_directory_done_workflow_file   = ${oozie_directory}/util/mark_directory_done/workflow.xml

archive_job_output_workflow_file    = ${oozie_directory}/util/archive_job_output/workflow.xml
# Workflow to send an error email
send_error_email_workflow_file      = ${oozie_directory}/util/send_error_email/workflow.xml

# HDFS path to hive-site.xml file.  This is needed to run hive actions.
hive_site_xml                       = ${name_node}/user/hive/hive-site.xml

# Table to write hourly mediacounts aggregates to
mediacounts_table                   = wmf.mediacounts

# HDFS path to directory where mediacounts data is time bucketed.
mediacounts_data_directory          = ${name_node}/wmf/data/wmf/mediacounts

# Temporary directory
temporary_directory                 = ${name_node}/wmf/tmp/analytics

# Archive base directory
archive_directory                   = ${name_node}/wmf/data/archive

# Archive directory for mediacounts
mediacounts_archive_directory       = ${archive_directory}/mediacounts

# Archive directory for daily mediacounts
mediacounts_daily_archive_directory = ${mediacounts_archive_directory}/daily

# SLA email to make sure we receive email if the job timeouts
sla_alert_contact                 = analytics-alerts@wikimedia.org

# Coordintator to start.
oozie.coord.application.path        = ${oozie_directory}/mediacounts/archive/coordinator.xml
oozie.use.system.libpath            = true
oozie.action.external.stats.write   = true
