# Hadoop properties.
name_node                           = hdfs://analytics-hadoop
job_tracker                         = resourcemanager.analytics.eqiad.wmnet:8032
hive_principal                      = hive/an-coord1001.eqiad.wmnet@WIKIMEDIA
hive2_jdbc_url                      = jdbc:hive2://an-coord1001.eqiad.wmnet:10000/default
hive_metastore_uri                  = thrift://an-coord1001.eqiad.wmnet:9083
hive_site_xml                       = ${name_node}/user/hive/hive-site.xml
queue_name                          = default
user                                = analytics
temp_directory                      = ${name_node}/tmp/${user}

# HDFS base path to refinery.
# When submitting this job for production, you should override this to point
# directly at a deployed directory, and not the symbolic 'current' directory.
# E.g.:  /wmf/refinery/2015-01-05T17.59.18Z--7bb7f07
refinery_directory                  = ${name_node}/wmf/refinery/current

# HDFS path to artifacts that will be used by this job.
# E.g. refinery-hive.jar should exist here.
artifacts_directory                 = ${refinery_directory}/artifacts

# HDFS base path to oozie files.
# Other files will be referenced relative to this path.
oozie_directory                     = ${refinery_directory}/oozie

# HDFS path to xml files.
bundle_file                         = ${oozie_directory}/data_quality_stats/${granularity}/bundle.xml
coordinator_file                    = ${oozie_directory}/data_quality_stats/${granularity}/coordinator.xml
workflow_file                       = ${oozie_directory}/data_quality_stats/workflow.xml

# Information about the destination data set.
data_quality_stats_table            = wmf.data_quality_stats
data_quality_stats_base_path        = ${name_node}/wmf/data/wmf/data_quality_stats
data_quality_stats_incoming_table   = ${user}.data_quality_stats_incoming

# Version of the refinery-source jar for query UDFs.
refinery_jar_version                = 0.0.93

# Spark options.
oozie_spark_lib                     = spark-2.4.4
# For generating report files.
updater_spark_master                = yarn
updater_spark_deploy                = cluster
updater_spark_job_name              = data-quality-stats-updater
updater_spark_job_class             = org.wikimedia.analytics.refinery.job.DataQualityStatsUpdater
updater_spark_job_jar               = ${artifacts_directory}/org/wikimedia/analytics/refinery/refinery-job-0.0.109.jar
updater_spark_driver_memory         = 2G
updater_spark_executor_memory       = 2G
updater_spark_executor_cores        = 2
updater_spark_max_num_executors     = 4

# Auxiliary workflow files.
send_error_email_workflow_file      = ${oozie_directory}/util/send_error_email/workflow.xml

# Default time to stop running this coordinator. Year 3000 == never!
stop_time                           = 3000-01-01T00:00Z

# SLA email to make sure we receive email if the job timeouts
sla_alert_contact                   = analytics-alerts@wikimedia.org

# Coordintator to start.
oozie.bundle.application.path       = ${bundle_file}
oozie.use.system.libpath            = true
oozie.action.external.stats.write   = true
