# Hadoop properties.
name_node                           = hdfs://analytics-hadoop
job_tracker                         = resourcemanager.analytics.eqiad.wmnet:8032
hive_principal                      = hive/analytics-hive.eqiad.wmnet@WIKIMEDIA
hive2_jdbc_url                      = jdbc:hive2://analytics-hive.eqiad.wmnet:10000/default
hive_metastore_uri                  = thrift://analytics-hive.eqiad.wmnet:9083
hive_site_xml                       = ${name_node}/user/hive/hive-site.xml
queue_name                          = default
user                                = analytics
# NOTE: temp_directory needs to be overriden when testing the job from a regular
# user (not analytics), as the default temp_directory value is not writable by
# other users than analytics.
temp_directory                      = ${name_node}/wmf/tmp/analytics

# HDFS base path to refinery.
# When submitting this job for production, you should override this to point
# directly at a deployed directory, and not the symbolic 'current' directory.
# E.g.:  /wmf/refinery/2015-01-05T17.59.18Z--7bb7f07
refinery_directory                  = ${name_node}/wmf/refinery/current

# HDFS path to artifacts that will be used by this job.
# E.g. refinery-hive.jar should exist here.
artifacts_directory                 = ${refinery_directory}/artifacts

# HDFS base path to oozie files.
# Other files will be referenced relative to this path.
oozie_directory                     = ${refinery_directory}/oozie

# HDFS path to xml files.
bundle_file                         = ${oozie_directory}/data_quality_stats/${granularity}/bundle.xml
coordinator_file                    = ${oozie_directory}/data_quality_stats/${granularity}/coordinator.xml
workflow_file                       = ${oozie_directory}/data_quality_stats/workflow.xml

# Information about the destination data set.
data_quality_stats_table            = wmf.data_quality_stats
data_quality_stats_base_path        = ${name_node}/wmf/data/wmf/data_quality_stats
data_quality_stats_incoming_table   = analytics.data_quality_stats_incoming

# Version of the refinery-source jar for query UDFs.
refinery_jar_version                = 0.0.136

# Spark options.
oozie_spark_lib                     = spark-2.4.4
spark_master                        = yarn
spark_deploy                        = cluster
spark_job_jar                       = ${artifacts_directory}/org/wikimedia/analytics/refinery/refinery-job-0.0.136.jar
spark_driver_memory                 = 2G
spark_executor_memory               = 2G
spark_executor_cores                = 2
spark_max_num_executors             = 4
# To update the data quality table.
updater_spark_job_name              = data-quality-stats-updater
updater_spark_job_class             = org.wikimedia.analytics.refinery.job.dataquality.DataQualityStatsUpdater
# To detect anomalies.
anomalies_spark_job_name            = data-quality-anomaly-detection
anomalies_spark_job_class           = org.wikimedia.analytics.refinery.job.dataquality.RSVDAnomalyDetection

# Auxiliary workflow files.
send_error_email_workflow_file      = ${oozie_directory}/util/send_error_email/workflow.xml

# Default time to stop running this coordinator. Year 3000 == never!
stop_time                           = 3000-01-01T00:00Z

# SLA email to make sure we receive email if the job timeouts
sla_alert_contact                   = analytics-alerts@wikimedia.org

# Coordintator to start.
oozie.bundle.application.path       = ${bundle_file}
oozie.use.system.libpath            = true
oozie.action.external.stats.write   = true
