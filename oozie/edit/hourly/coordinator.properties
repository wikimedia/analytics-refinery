# Hadoop properties.
name_node                           = hdfs://analytics-hadoop
job_tracker                         = resourcemanager.analytics.eqiad.wmnet:8032
queue_name                          = default
hive_principal                      = hive/analytics-hive.eqiad.wmnet@WIKIMEDIA
hive2_jdbc_url                      = jdbc:hive2://analytics-hive.eqiad.wmnet:10000/default
user                                = analytics

# HDFS base path to refinery.
# When submitting this job for production, you should override this to point
# directly at a deployed directory, and not the symbolic 'current' directory.
# E.g.:  /wmf/refinery/2015-01-05T17.59.18Z--7bb7f07
refinery_directory                  = ${name_node}/wmf/refinery/current

# HDFS base path to oozie files.
# Other files will be referenced relative to this path.
oozie_directory                     = ${refinery_directory}/oozie

# HDFS path to edit_hourly coordinator.
coordinator_file                    = ${oozie_directory}/edit/hourly/coordinator.xml

# HDFS path to edit_hourly workflow.
workflow_file                       = ${oozie_directory}/edit/hourly/workflow.xml

# HDFS path to mediawiki history dataset definitions.
mediawiki_history_datasets_file     = ${oozie_directory}/mediawiki/history/datasets.xml
mw_directory                        = ${name_node}/wmf/data/wmf/mediawiki

# HDFS path to edit dataset definitions.
edit_datasets_file                  = ${oozie_directory}/edit/datasets.xml
edit_data_directory                 = ${name_node}/wmf/data/wmf/edit

# Initial import time of the webrequest dataset.
start_time                          = 2019-03-01T00:00Z

# Time to stop running this coordinator. Year 3000 == never!
stop_time                           = 3000-01-01T00:00Z

# Workflow to flag a directory as done.
flag_directory_done_workflow_file   = ${oozie_directory}/util/mark_directory_done/workflow.xml
# Workflow to send an error email.
send_error_email_workflow_file      = ${oozie_directory}/util/send_error_email/workflow.xml

# HDFS path to hive-site.xml file.
hive_site_xml                       = ${name_node}/user/hive/hive-site.xml

# Fully qualified Hive table names.
mediawiki_history_table             = wmf.mediawiki_history
edit_hourly_table                   = wmf.edit_hourly
wiki_map_table                      = canonical_data.wikis

# SLA email to make sure we receive email if the job timeouts
sla_alert_contact                 = analytics-alerts@wikimedia.org

# Coordintator to start.
oozie.coord.application.path        = ${coordinator_file}
oozie.use.system.libpath            = true
oozie.action.external.stats.write   = true
