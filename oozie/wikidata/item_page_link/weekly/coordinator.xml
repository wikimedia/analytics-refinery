<?xml version="1.0" encoding="UTF-8"?>
<!-- FIXME Deprecated by hql/wikidata/item_page_link/weekly -->
<coordinator-app xmlns="uri:oozie:coordinator:0.4"
    xmlns:sla="uri:oozie:sla:0.2"
    name="wikidata-item_page_link-weekly-coord"
    frequency="${coord:days(7)}"
    start="${start_time}"
    end="${stop_time}"
    timezone="Universal">

    <parameters>
        <!-- Required properties -->
        <property><name>queue_name</name></property>
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive2_jdbc_url</name></property>
        <property><name>hive_metastore_uri</name></property>
        <property><name>workflow_file</name></property>
        <property><name>start_time</name></property>
        <property><name>stop_time</name></property>
        <property><name>hive_site_xml</name></property>

        <property><name>oozie_spark_lib</name></property>
        <property><name>artifacts_directory</name></property>
        <property><name>spark_master</name></property>
        <property><name>spark_assembly_zip</name></property>
        <property><name>spark_job_jar</name></property>
        <property><name>spark_job_class</name></property>
        <property><name>spark_executor_memory</name></property>
        <property><name>spark_executor_cores</name></property>
        <property><name>spark_driver_memory</name></property>
        <property><name>spark_max_num_executors</name></property>

        <property><name>wikidata_datasets_file</name></property>
        <property><name>wikidata_data_directory</name></property>
        <property><name>mw_datasets_file</name></property>
        <property><name>mw_directory</name></property>
        <property><name>mw_raw_datasets_file</name></property>
        <property><name>mw_raw_directory</name></property>
        <property><name>event_datasets_file</name></property>
        <property><name>event_data_directory</name></property>

        <property><name>wikidata_entity_table</name></property>
        <property><name>mw_page_history_table</name></property>
        <property><name>event_page_move_table</name></property>
        <property><name>mw_project_namespace_map_table</name></property>
        <property><name>wikidata_item_page_link_table</name></property>

        <property><name>wikidata_item_page_link_work_partitions</name></property>
        <property><name>wikidata_item_page_link_output_partitions</name></property>

        <property><name>add_partition_workflow_file</name></property>
        <property><name>mark_directory_done_workflow_file</name></property>
        <property><name>send_error_email_workflow_file</name></property>

        <property><name>sla_alert_contact</name></property>
    </parameters>

    <controls>
        <!--
        This timeout prevents the job from waiting indefinietly if the data the job
        depends is never generated (dumps or events). The job will fail after waiting
        for 15 days (same duration as SLA + 1 day), allowing the team to get notified
        and take approprite action (job being stuck in waiting can easily go unnoticed).

        (timeout is measured in minutes)
        -->
        <timeout>21600</timeout>

        <!-- Setting low concurrency for resource sharing.
             The job runs pretty fast (~1 minute) and increasing concurrency should not cause any problems-->
        <concurrency>1</concurrency>

        <throttle>2</throttle>

    </controls>

    <datasets>
        <!--
        Include wikidata, mw and mw_raw datasets files.
        -->
        <include>${wikidata_datasets_file}</include>
        <include>${mw_datasets_file}</include>
        <include>${mw_raw_datasets_file}</include>
        <include>${event_datasets_file}</include>
    </datasets>

    <input-events>
        <data-in name="wikidata_entity_partitioned" dataset="wikidata_entity_partitioned">
            <instance>${coord:current(0)}</instance>
        </data-in>

        <!--
          The next 2 input datasets are partitioned monthly
          Data up to month N being available at snapshot N-1, we need to shift
          the instance by 1 month in the past (when current(0) is 2020-03-02 for
          instance, we want the next 2 dataset instances being 2020-02).
        -->
        <data-in name="mw_page_history_partitioned" dataset="mw_page_history_partitioned">
            <instance>${coord:offset(-1, "MONTH")}</instance>
        </data-in>
        <data-in name="mw_project_namespace_map_partitioned" dataset="mw_project_namespace_map_partitioned">
            <instance>${coord:offset(-1, "MONTH")}</instance>
        </data-in>

        <!--
          We need to include data from the history snapshot to the end of the wikidata entity
          snapshot, so for 2020-03-02 for instance, we want 2020-03-01 to 2020-03-09.
          However, due to limitations in Oozie, we have to approximate this with -1 month to +7 days
        -->
        <data-in name="event_mediawiki_page_move" dataset="event_mediawiki_page_move">
            <start-instance>${coord:offset(-1, "MONTH")}</start-instance>
            <end-instance>${coord:offset(7, "DAY")}</end-instance>
        </data-in>
    </input-events>

    <output-events>
        <data-out name="wikidata_item_page_link_parquet" dataset="wikidata_item_page_link_parquet">
            <instance>${coord:current(0)}</instance>
        </data-out>

        <data-out name="wikidata_item_page_link_partitioned" dataset="wikidata_item_page_link_partitioned">
            <instance>${coord:current(0)}</instance>
        </data-out>
    </output-events>

    <action>
        <workflow>
            <app-path>${workflow_file}</app-path>
            <configuration>
                <property>
                    <name>wikidata_snapshot</name>
                    <value>${coord:formatTime(coord:nominalTime(), "yyyy")}-${coord:formatTime(coord:nominalTime(), "MM")}-${coord:formatTime(coord:nominalTime(), "dd")}</value>
                </property>
                <property>
                    <name>history_snapshot</name>
                    <value>${coord:formatTime(coord:dateOffset(coord:nominalTime(), -1, "MONTH"), "yyyy")}-${coord:formatTime(coord:dateOffset(coord:nominalTime(), -1, "MONTH"), "MM")}</value>
                </property>
                <property>
                    <name>wikidata_item_page_link_parquet_location</name>
                    <value>${coord:dataOut('wikidata_item_page_link_parquet')}</value>
                </property>
            </configuration>
        </workflow>

        <sla:info>
            <!--
                Use action actual time as SLA base, since it's the time used
                to compute timeout
                Job is waiting for the week data to be computed and copied to HDFS
                which happens more or less 4 days after the actual week-day.
                We put 14 days to only alert if the computation has not happened
                when the next job materializes.
            -->
            <sla:nominal-time>${coord:actualTime()}</sla:nominal-time>
            <sla:should-end>${14 * DAYS}</sla:should-end>
            <sla:alert-events>end_miss</sla:alert-events>
            <sla:alert-contact>${sla_alert_contact}</sla:alert-contact>
        </sla:info>

    </action>
</coordinator-app>
