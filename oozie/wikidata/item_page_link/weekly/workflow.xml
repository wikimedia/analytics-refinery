<?xml version="1.0" encoding="UTF-8"?>
<!-- FIXME Deprecated by hql/wikidata/item_page_link/weekly -->
<workflow-app xmlns="uri:oozie:workflow:0.4"
    name="wikidata-item_page_link-weekly-wf-${wikidata_snapshot}">

    <parameters>

        <!-- Allows changing spark version to be used by oozie -->
        <property>
            <name>oozie.action.sharelib.for.spark</name>
            <value>${oozie_spark_lib}</value>
        </property>

        <!-- Default values for inner oozie settings -->
        <property>
            <name>oozie_launcher_queue_name</name>
            <value>${queue_name}</value>
        </property>
        <property>
            <name>oozie_launcher_memory</name>
            <value>2048</value>
        </property>

        <!-- Required properties -->
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>queue_name</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive2_jdbc_url</name></property>
        <property><name>hive_metastore_uri</name></property>

        <property>
            <name>hive_site_xml</name>
            <description>hive-site.xml file path in HDFS</description>
        </property>
        <property>
            <name>spark_master</name>
            <description>Master to be used for Spark (yarn, local, other)</description>
        </property>
        <property>
            <name>spark_assembly_zip</name>
            <description>The spark assembly zip file on HDFS, preventing to repackage it everytime</description>
        </property>
        <property>
            <name>spark_job_jar</name>
            <description>Path to the jar to be used to run spark job</description>
        </property>
        <property>
            <name>spark_job_class</name>
            <description>Class of the spark job to be run</description>
        </property>
        <property>
            <name>spark_executor_memory</name>
            <description>Memory to allocate for each spark executor</description>
        </property>
        <property>
            <name>spark_executor_cores</name>
            <description>Number of cores to allocate for each spark executor</description>
        </property>
        <property>
            <name>spark_driver_memory</name>
            <description>Memory to allocate for spark driver process</description>
        </property>
        <property>
            <name>spark_max_num_executors</name>
            <description>Maximum number of concurrent executors for spark with dynamic allocation</description>
        </property>

        <property>
            <name>wikidata_entity_table</name>
            <description>The wikidata_entity table in hive as source</description>
        </property>
        <property>
            <name>mw_page_history_table</name>
            <description>The mediawiki_page_history table in hive as source</description>
        </property>
        <property>
            <name>event_page_move_table</name>
            <description>The mediawiki_page_move table in hive as source</description>
        </property>
        <property>
            <name>mw_project_namespace_map_table</name>
            <description>The mediawiki_project_namespace_map table in hive as source</description>
        </property>
        <property>
            <name>wikidata_item_page_link_table</name>
            <description>The wikidata_item_page_link table in hive as destination</description>
        </property>

        <property>
            <name>wikidata_snapshot</name>
            <description>The wikidata weekly snapshot currently worked (YYYY-MM-DD format)</description>
        </property>
        <property>
            <name>history_snapshot</name>
            <description>The history monthly snapshot currently worked (YYYY-MM format)</description>
        </property>
        <property>
            <name>wikidata_item_page_link_parquet_location</name>
            <description>The path of the generated wikidata_item_page_link parquet files</description>
        </property>

        <property>
            <name>wikidata_item_page_link_work_partitions</name>
            <description>Number of partitions to use to compute</description>
        </property>
        <property>
            <name>wikidata_item_page_link_output_partitions</name>
            <description>Number of parquet files to write</description>
        </property>

        <!-- Subworkflows -->
        <property>
            <name>add_partition_workflow_file</name>
            <description>Workflow definition for adding a partition</description>
        </property>
        <property>
            <name>mark_directory_done_workflow_file</name>
            <description>Workflow for marking a directory done</description>
        </property>
        <property>
            <name>send_error_email_workflow_file</name>
            <description>Workflow for sending an email</description>
        </property>

    </parameters>

    <credentials>
        <credential name="hcat-cred" type="hcat">
            <property>
                <name>hcat.metastore.principal</name>
                <value>${hive_principal}</value>
            </property>
            <property>
               <name>hcat.metastore.uri</name>
               <value>${hive_metastore_uri}</value>
            </property>
        </credential>
    </credentials>


    <start to="compute_item_page_link"/>

    <action name="compute_item_page_link" cred="hcat-cred">
        <spark xmlns="uri:oozie:spark-action:0.1">
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <configuration>
                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>
            </configuration>
            <master>${spark_master}</master>
            <mode>${spark_deploy}</mode>
            <name>${spark_job_name}-${wikidata_snapshot}</name>
            <class>${spark_job_class}</class>
            <jar>${spark_job_jar}</jar>
            <spark-opts>--executor-memory ${spark_executor_memory} --executor-cores ${spark_executor_cores} --driver-memory ${spark_driver_memory} --queue ${queue_name} --conf spark.dynamicAllocation.maxExecutors=${spark_max_num_executors}</spark-opts>
            <arg>--output-path</arg>
            <arg>${wikidata_item_page_link_parquet_location}</arg>
            <arg>--wikidata-entity-table</arg>
            <arg>${wikidata_entity_table}</arg>
            <arg>--namespace-table</arg>
            <arg>${mw_project_namespace_map_table}</arg>
            <arg>--page-history-table</arg>
            <arg>${mw_page_history_table}</arg>
            <arg>--page-move-table</arg>
            <arg>${event_page_move_table}</arg>
            <arg>--wikidata-snapshot</arg>
            <arg>${wikidata_snapshot}</arg>
            <arg>--history-snapshot</arg>
            <arg>${history_snapshot}</arg>
            <arg>--num-work-partitions</arg>
            <arg>${wikidata_item_page_link_work_partitions}</arg>
            <arg>--num-output-partitions</arg>
            <arg>${wikidata_item_page_link_output_partitions}</arg>
        </spark>
        <ok to="add_wikidata_item_page_link_partition" />
        <error to="send_error_email" />
    </action>

    <action name="add_wikidata_item_page_link_partition">
        <sub-workflow>
            <app-path>${add_partition_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>table</name>
                    <value>${wikidata_item_page_link_table}</value>
                </property>
                <property>
                    <name>location</name>
                    <value>${wikidata_item_page_link_parquet_location}</value>
                </property>
                <property>
                    <name>partition_spec</name>
                    <value>snapshot='${wikidata_snapshot}'</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="mark_wikidata_entity_hive_done"/>
        <error to="send_error_email"/>
    </action>

    <action name="mark_wikidata_entity_hive_done">
        <sub-workflow>
            <app-path>${mark_directory_done_workflow_file}</app-path>
            <configuration>
                <property>
                    <name>directory</name>
                    <value>${wikidata_item_page_link_parquet_location}</value>
                </property>
                <property>
                    <name>done_file</name>
                    <value>_PARTITIONED</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="end"/>
        <error to="send_error_email"/>
    </action>

    <action name="send_error_email">
        <sub-workflow>
            <app-path>${send_error_email_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>parent_name</name>
                    <value>${wf:name()}</value>
                </property>
                <property>
                    <name>parent_failed_action</name>
                    <value>${wf:lastErrorNode()}</value>
                </property>
                <property>
                    <name>parent_error_code</name>
                    <value>${wf:errorCode(wf:lastErrorNode())}</value>
                </property>
                <property>
                    <name>parent_error_message</name>
                    <value>${wf:errorMessage(wf:lastErrorNode())}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="kill"/>
        <error to="kill"/>
    </action>

    <kill name="kill">
        <message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name="end"/>
</workflow-app>
