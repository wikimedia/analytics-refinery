<?xml version="1.0" encoding="UTF-8"?>
<coordinator-app xmlns="uri:oozie:coordinator:0.4"
    xmlns:sla="uri:oozie:sla:0.2"
    name="wikidata-json_entity-weekly-coord"
    frequency="${coord:days(7)}"
    start="${start_time}"
    end="${stop_time}"
    timezone="Universal">

    <parameters>
        <!-- Required properties -->
        <property><name>queue_name</name></property>
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive2_jdbc_url</name></property>
        <property><name>hive_metastore_uri</name></property>
        <property><name>workflow_file</name></property>
        <property><name>start_time</name></property>
        <property><name>stop_time</name></property>
        <property><name>hive_site_xml</name></property>

        <property><name>oozie_spark_lib</name></property>
        <property><name>artifacts_directory</name></property>
        <property><name>spark_master</name></property>
        <property><name>spark_assembly_zip</name></property>
        <property><name>spark_job_jar</name></property>
        <property><name>spark_job_class</name></property>
        <property><name>spark_executor_memory</name></property>
        <property><name>spark_executor_cores</name></property>
        <property><name>spark_driver_memory</name></property>
        <property><name>spark_max_num_executors</name></property>

        <property><name>datasets_file</name></property>
        <property><name>wikidata_data_directory</name></property>
        <property><name>datasets_raw_file</name></property>
        <property><name>wikidata_raw_data_directory</name></property>

        <property><name>wikidata_entity_table</name></property>

        <property><name>wikidata_entity_parquet_partitions</name></property>

        <property><name>add_partition_workflow_file</name></property>
        <property><name>mark_directory_done_workflow_file</name></property>
        <property><name>send_error_email_workflow_file</name></property>

        <property><name>sla_alert_contact</name></property>
    </parameters>

    <controls>
        <!--
        This timeout prevents the job from waiting indefinietly if the data the job
        depends is never generated (dumps or events). The job will fail after waiting
        for 15 days (same duration as SLA + 1 day), allowing the team to get notified
        and take approprite action (job being stuck in waiting can easily go unnoticed).

        (timeout is measured in minutes)
        -->
        <timeout>21600</timeout>

        <!-- Setting low concurrency for resource sharing.
             The job runs pretty fast (~1 minute) and increasing concurrency should not cause any problems-->
        <concurrency>1</concurrency>

        <throttle>2</throttle>

    </controls>

    <datasets>
        <!--
        Include refined and raw datasets files.
        -->
        <include>${datasets_file}</include>
        <include>${datasets_raw_file}</include>
    </datasets>

    <input-events>
        <data-in name="wikidata_all_json_raw_dump" dataset="wikidata_all_json_raw_dump">
            <!--
              Use a one week shift in input-event as dump generated at week YYYYMMDD
              contains data for the week up-to DD, and we want to name it snapshot=YYYY-MM-(DD-7)
              Note: coord:current(1) shifts 7 days as the dataset uses 7 days frequency
            -->
            <instance>${coord:current(1)}</instance>
        </data-in>
    </input-events>

    <output-events>
        <data-out name="wikidata_entity_parquet" dataset="wikidata_entity_parquet">
            <instance>${coord:current(0)}</instance>
        </data-out>

        <data-out name="wikidata_entity_partitioned" dataset="wikidata_entity_partitioned">
            <instance>${coord:current(0)}</instance>
        </data-out>
    </output-events>

    <action>
        <workflow>
            <app-path>${workflow_file}</app-path>
            <configuration>
                <property>
                    <name>snapshot</name>
                    <value>${coord:formatTime(coord:nominalTime(), "yyyy")}-${coord:formatTime(coord:nominalTime(), "MM")}-${coord:formatTime(coord:nominalTime(), "dd")}</value>
                </property>
                <property>
                    <name>wikidata_json_dumps_location</name>
                    <value>${coord:dataIn('wikidata_all_json_raw_dump')}</value>
                </property>
                <property>
                    <name>wikidata_entity_parquet_location</name>
                    <value>${coord:dataOut('wikidata_entity_parquet')}</value>
                </property>
            </configuration>
        </workflow>

        <sla:info>
            <!--
                Use action actual time as SLA base, since it's the time used
                to compute timeout
                Job is waiting for the week data to be computed and copied to
                hdfs which happens more or less 4 days after the actual week-day.
                We put 14 days to only alert if the computation has not happened
                when the next job materializes.
            -->
            <sla:nominal-time>${coord:actualTime()}</sla:nominal-time>
            <sla:should-end>${14 * DAYS}</sla:should-end>
            <sla:alert-events>end_miss</sla:alert-events>
            <sla:alert-contact>${sla_alert_contact}</sla:alert-contact>
        </sla:info>

    </action>
</coordinator-app>