<?xml version="1.0" encoding="UTF-8"?>
<coordinator-app xmlns="uri:oozie:coordinator:0.4"
    xmlns:sla="uri:oozie:sla:0.2"
    name="wikidata-coeditors_metrics-coord"
    frequency="${coord:months(1)}"
    start="${start_time}"
    end="${stop_time}"
    timezone="Universal">

    <parameters>

        <!-- Required properties. -->
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive_metastore_uri</name></property>
        <property><name>queue_name</name></property>

        <property><name>workflow_file</name></property>
        <property><name>dataset_raw_file</name></property>
        <property><name>mw_raw_directory</name></property>
        <property><name>dataset_file</name></property>
        <property><name>mw_directory</name></property>

        <property><name>start_time</name></property>
        <property><name>stop_time</name></property>

        <property><name>mw_history_table</name></property>
        <property><name>mw_project_namespace_table</name></property>

        <property><name>spark_master</name></property>
        <property><name>spark_job_jar</name></property>
        <property><name>spark_job_class</name></property>
        <property><name>spark_executor_memory</name></property>
        <property><name>spark_driver_memory</name></property>
        <property><name>graphite_host</name></property>
        <property><name>graphite_port</name></property>
        <property><name>graphite_namespace</name></property>

        <property><name>error_emails_recipients</name></property>
        <property><name>send_error_email_workflow_file</name></property>
        <property><name>sla_alert_contact</name></property>
    </parameters>

    <controls>
        <!--(timeout is measured in minutes)-->
        <timeout>-1</timeout>

        <!-- Setting low concurrency for resource sharing.
             The job runs pretty fast (~1 minute) and increasing concurrency should not cause any problems-->
        <concurrency>2</concurrency>

        <throttle>2</throttle>

    </controls>

    <datasets>
        <!--
        Include datasets files.
        $dataset_file will be used as the input events
        -->
        <include>${dataset_raw_file}</include>
        <include>${dataset_file}</include>
    </datasets>

    <input-events>
        <data-in name="mw_project_namespace_map_partitioned" dataset="mw_project_namespace_map_partitioned">
            <instance>${coord:current(0)}</instance>
        </data-in>
        <data-in name="mw_denormalized_history_partitioned" dataset="mw_denormalized_history_partitioned">
            <instance>${coord:current(0)}</instance>
        </data-in>
    </input-events>

    <action>
        <workflow>
            <app-path>${workflow_file}</app-path>
            <configuration>
                <property>
                    <name>year</name>
                    <value>${coord:formatTime(coord:nominalTime(), "yyyy")}</value>
                </property>
                <property>
                    <name>month</name>
                    <value>${coord:formatTime(coord:nominalTime(), "MM")}</value>
                </property>
            </configuration>
        </workflow>

        <sla:info>
            <!--
                Use action actual time as SLA base, since it's the time used
                to compute timeout
                We use 40 days as mediawiki-denormalize is needed an can
                be up to 39 days late
            -->
            <sla:nominal-time>${coord:actualTime()}</sla:nominal-time>
            <sla:should-end>${40 * DAYS}</sla:should-end>
            <sla:alert-events>end_miss</sla:alert-events>
            <sla:alert-contact>${sla_alert_contact}</sla:alert-contact>
        </sla:info>
    </action>
</coordinator-app>
