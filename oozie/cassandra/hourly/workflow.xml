<?xml version="1.0" encoding="UTF-8"?>
<workflow-app xmlns="uri:oozie:workflow:0.4"
    name="cassandra-hourly-wf-${cassandra_keyspace}-${year}-${month}-${day}-${hour}">

    <parameters>

        <!-- Default values for inner oozie settings -->
        <property>
            <name>oozie_launcher_queue_name</name>
            <value>${queue_name}</value>
        </property>
        <property>
            <name>oozie_launcher_memory</name>
            <value>2048</value>
        </property>

        <!-- Required properties -->
        <property><name>queue_name</name></property>
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive2_jdbc_url</name></property>

        <property>
            <name>hive_script</name>
            <description>Hive script to run.</description>
        </property>
        <property>
            <name>hive_site_xml</name>
            <description>hive-site.xml file path in HDFS</description>
        </property>
        <property>
            <name>source_table</name>
            <description>Hive table to use</description>
        </property>
        <property>
            <name>temporary_directory</name>
            <description>A temporary directory to store data.</description>
        </property>
        <property>
            <name>send_error_email_workflow_file</name>
            <description>Workflow for sending an email</description>
        </property>

        <!-- cassandra loader -->
        <property>
            <name>refinery_cassandra_jar_path</name>
            <description>The refinery-cassandra jar file path in HDFS</description>
        </property>
        <property>
            <name>cassandra_reducer_class</name>
            <description>The reducer class to be used in refinery-cassandra jar</description>
        </property>
        <property>
            <name>cassandra_output_format_class</name>
            <description>The output format class to be used in refinery-cassandra jar</description>
        </property>
        <property>
            <name>cassandra_parallel_loaders</name>
            <description>The number of reducers to parallel load cassandra</description>
        </property>
        <property>
            <name>cassandra_nodes</name>
            <description>The number of nodes of the cassandra cluster</description>
        </property>
        <property>
            <name>batch_size</name>
            <description>The size of a batch sent to a node for insertion</description>
        </property>

        <!-- cassandra settings -->
        <property>
            <name>cassandra_host</name>
            <description>The cassandra host to connect for load</description>
        </property>
        <property>
            <name>cassandra_port</name>
            <description>The cassandra port to connect for load</description>
        </property>
        <property>
            <name>cassandra_username</name>
            <description>The cassandra username for load</description>
        </property>
        <property>
            <name>cassandra_password</name>
            <description>The cassandra password for load</description>
        </property>
        <property>
            <name>cassandra_write_consistency</name>
            <description>The cassandra write consistency level</description>
        </property>

        <!-- Cassandra load job parameters-->
        <property>
            <name>hive_fields</name>
            <description>Hive output fields names</description>
        </property>
        <property>
            <name>hive_fields_types</name>
            <description>Hive output fields types</description>
        </property>

        <property>
            <name>cassandra_cql</name>
            <description>The cassandra CQL used to load</description>
        </property>
        <property>
            <name>cassandra_keyspace</name>
            <description>Keyspace to load</description>
        </property>
        <property>
            <name>cassandra_table</name>
            <description>Table to load</description>
        </property>
        <property>
            <name>cassandra_fields</name>
            <description>Fields from hive_fields to be loaded</description>
        </property>
        <property>
            <name>cassandra_primary_keys</name>
            <description>Primary keys name in cassandra</description>
        </property>

        <property>
            <name>constant_output_domain_value</name>
            <description>Constant value for cassandra _domain field</description>
        </property>
        <property>
            <name>constant_output_granularity_value</name>
            <description>Constant value for cassandra granularity field</description>
        </property>
        <property>
            <name>constant_output_tid_value</name>
            <description>Constant value for cassandra _tid field</description>
        </property>

        <!-- Cassandra 3 spark load parameters -->
        <property><name>hive_metastore_uri</name></property>
        <property>
            <name>oozie.action.sharelib.for.spark</name>
            <value>${oozie_spark_lib}</value>
        </property>

        <property>
            <name>spark_master</name>
            <description>Master to be used for Spark (yarn, local, other)</description>
        </property>
        <property>
            <name>spark_deploy</name>
            <description>Spark deploy-mode for the job (cluster or client)</description>
        </property>
        <property>
            <name>spark_job_name</name>
            <description>Spark job name to be used</description>
        </property>
        <property>
            <name>spark_assembly_zip</name>
            <description>The spark assembly zip file on HDFS, preventing to repackage it everytime</description>
        </property>
        <property>
            <name>spark_job_jar</name>
            <description>Path to the jar to be used to run spark job</description>
        </property>
        <property>
            <name>spark_job_class</name>
            <description>Class of the spark job to be run</description>
        </property>
        <property>
            <name>spark_executor_memory</name>
            <description>Memory to allocate for each spark executor</description>
        </property>
        <property>
            <name>spark_executor_cores</name>
            <description>CPU-Cores to allocate for each spark executor</description>
        </property>
        <property>
            <name>spark_driver_memory</name>
            <description>Memory to allocate for spark driver process</description>
        </property>
        <property>
            <name>spark_max_executors</name>
            <description>Maximum number of executors for spark</description>
        </property>

        <property>
            <name>cassandra3_host</name>
            <description>The cassandra3 host to connect for load</description>
        </property>
        <property>
            <name>cassandra3_username</name>
            <description>The cassandra3 username for load</description>
        </property>
        <property>
            <name>cassandra3_password</name>
            <description>The cassandra3 password for load</description>
        </property>
        <property>
            <name>cassandra3_columns</name>
            <description>Cassandra3 columns to load (same order as in the HQL)</description>
        </property>
        <property>
            <name>cassandra3_hql</name>
            <description>HQL to get the data to be loaded on cassandra3</description>
        </property>

        <!-- Time parameters-->
        <property>
            <name>year</name>
            <description>The partition's year</description>
        </property>
        <property>
            <name>month</name>
            <description>The partition's month</description>
        </property>
        <property>
            <name>day</name>
            <description>The partition's day</description>
        </property>
        <property>
            <name>hour</name>
            <description>The partition's hour</description>
        </property>

    </parameters>

    <credentials>
        <credential name="hive2-cred" type="hive2">
            <property>
                <name>hive2.server.principal</name>
                <value>${hive_principal}</value>
            </property>
            <property>
               <name>hive2.jdbc.url</name>
               <value>${hive2_jdbc_url}</value>
            </property>
        </credential>
        <credential name="hcat-cred" type="hcat">
            <property>
                <name>hcat.metastore.principal</name>
                <value>${hive_principal}</value>
            </property>
            <property>
               <name>hcat.metastore.uri</name>
               <value>${hive_metastore_uri}</value>
            </property>
        </credential>
    </credentials>


    <start to="prepare_data"/>

    <action name="prepare_data" cred="hive2-cred">
        <hive2 xmlns="uri:oozie:hive2-action:0.2">
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <job-xml>${hive_site_xml}</job-xml>
            <configuration>
                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>
                <!--Let hive decide on the number of reducers -->
                <property>
                    <name>mapred.reduce.tasks</name>
                    <value>-1</value>
                </property>
                <property>
                    <name>hive.exec.scratchdir</name>
                    <value>/tmp/hive-${user}</value>
                </property>
            </configuration>
            <jdbc-url>${hive2_jdbc_url}</jdbc-url>
            <script>${hive_script}</script>

            <!-- Query parameters -->
            <param>source_table=${source_table}</param>
            <param>separator=${hive_value_separator}</param>
            <param>destination_directory=${temporary_directory}/${wf:id()}-${cassandra_keyspace}</param>
            <param>year=${year}</param>
            <param>month=${month}</param>
            <param>day=${day}</param>
            <param>hour=${hour}</param>

            <!-- Beeline arguments -->
            <argument>--verbose</argument>
            <argument>--hiveconf</argument>
            <argument>mapreduce.job.queuename=${queue_name}</argument>
        </hive2>

        <ok to="load_cassandra"/>
        <error to="send_error_email"/>
    </action>

    <action name="load_cassandra">
        <map-reduce>
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <configuration>

                <!-- Global Configuration -->

                <!-- Ensure classpath jars are loading in correct order-->
                <property>
                    <name>mapreduce.job.user.classpath.first</name>
                    <value>true</value>
                </property>

                <!--Set queue -->
                <property>
                    <name>mapreduce.job.queuename</name>
                    <value>${queue_name}</value>
                </property>

                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>

                <!-- Set mapper and reducer to yarn api style-->
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>

                <!-- Set logging to INFO -->
                <property>
                    <name>mapreduce.map.log.level</name>
                    <value>INFO</value>
                </property>
                <property>
                    <name>mapreduce.reduce.log.level</name>
                    <value>INFO</value>
                </property>


                <!-- Map side -->
                <!-- Default mapper - Nothing done -->
                <property>
                    <name>mapreduce.map.class</name>
                    <value>org.apache.hadoop.mapreduce.Mapper</value>
                </property>
                <!-- compress map output -->
                <property>
                    <name>mapreduce.map.output.compress</name>
                    <value>true</value>
                </property>
                <!-- input format and classes -->
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.TextInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.input.key.class</name>
                    <value>org.apache.hadoop.io.LongWritable</value>
                </property>
                <property>
                    <name>mapreduce.job.input.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <!-- map output classes-->
                <property>
                    <name>mapreduce.map.output.key.class</name>
                    <value>org.apache.hadoop.io.LongWritable</value>
                </property>
                <property>
                    <name>mapreduce.map.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <!-- input dir -->
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>${temporary_directory}/${wf:id()}-${cassandra_keyspace}</value>
                </property>


                <!-- Reduce side -->
                <!-- Load to Cassandra reducer with X reducers-->
                <property>
                    <name>mapreduce.reduce.class</name>
                    <value>${cassandra_reducer_class}</value>
                </property>
                <property>
                    <name>mapreduce.job.reduces</name>
                    <value>${cassandra_parallel_loaders}</value>
                </property>
                <property>
                    <name>mapreduce.reduce.speculative</name>
                    <value>false</value>
                </property>

                <!-- Cassandra output format and classes -->
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>${cassandra_output_format_class}</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>java.util.Map</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>java.util.List</value>
                </property>

                <!-- Cassandra reducer core config-->
                <!-- Cassandra host - legacy naming-->
                <property>
                    <name>cassandra.output.thrift.address</name>
                    <value>${cassandra_host}</value>
                </property>
                <property>
                    <name>cassandra.output.native.port</name>
                    <value>${cassandra_port}</value>
                </property>
                <property>
                    <name>cassandra.input.native.auth.provider</name>
                    <value>com.datastax.driver.core.PlainTextAuthProvider</value>
                </property>
                <property>
                    <name>cassandra.username</name>
                    <value>${cassandra_username}</value>
                </property>
                <property>
                    <name>cassandra.password</name>
                    <value>${cassandra_password}</value>
                </property>
                <property>
                    <!--Write consistency level -->
                    <name>cassandra.consistencylevel.write</name>
                    <value>${cassandra_write_consistency}</value>
                </property>
                <property>
                    <!--Quoted on purpose -->
                    <name>cassandra.output.keyspace</name>
                    <value>"${cassandra_keyspace}"</value>
                </property>
                <property>
                    <!-- actually used for column familly - quoted on purpose-->
                    <name>mapreduce.output.basename</name>
                    <value>"${cassandra_table}"</value>
                </property>
                <property>
                    <name>cassandra.output.partitioner.class</name>
                    <value>Murmur3Partitioner</value>
                </property>
                <property>
                    <name>cassandra.output.cql</name>
                    <value>${cassandra_cql}</value>
                </property>
                <property>
                    <name>mapreduce.output.columnfamilyoutputformat.batch.threshold</name>
                    <value>${batch_size}</value>
                </property>
                <property>
                    <name>mapreduce.output.columnfamilyoutputformat.queue.size</name>
                    <value>${batch_size * cassandra_nodes}</value>
                </property>


                <!-- Cassandra reducer specific config-->
                <property>
                    <name>input_separator</name>
                    <value>${hive_value_separator}</value>
                </property>
                <property>
                    <name>input_fields</name>
                    <value>${hive_fields}</value>
                </property>
                <property>
                    <name>input_fields_types</name>
                    <value>${hive_fields_types}</value>
                </property>
                <property>
                    <name>output_fields</name>
                    <value>${cassandra_fields}</value>
                </property>
                <property>
                    <name>output_primary_keys</name>
                    <value>${cassandra_primary_keys}</value>
                </property>
                <property>
                    <name>${constant_output_domain_field}</name>
                    <value>${constant_output_domain_value}</value>
                </property>
                <property>
                    <name>${constant_output_granularity_field}</name>
                    <value>${constant_output_granularity_value}</value>
                </property>
                <property>
                    <name>${constant_output_tid_field}</name>
                    <value>${constant_output_tid_value}</value>
                </property>
            </configuration>
            <archive>${refinery_cassandra_jar_path}</archive>

        </map-reduce>
        <ok to="remove_temporary_data"/>
        <error to="send_error_email"/>
    </action>

    <action name="remove_temporary_data">
        <fs>
            <delete path="${temporary_directory}/${wf:id()}-${cassandra_keyspace}"/>
        </fs>
        <ok to="load_cassandra3_spark"/>
        <error to="send_error_email"/>
    </action>

    <action name="load_cassandra3_spark" cred="hcat-cred">
        <spark xmlns="uri:oozie:spark-action:0.1">

            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <configuration>
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>
            </configuration>
            <master>${spark_master}</master>
            <mode>${spark_deploy}</mode>
            <name>${spark_job_name}-${year}-${month}-${day}-${hour}</name>
            <class>${spark_job_class}</class>
            <jar>${spark_job_jar}</jar>
            <spark-opts>--conf spark.yarn.archive=${spark_assembly_zip} --conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.enabled=true --executor-memory ${spark_executor_memory} --executor-cores ${spark_executor_cores} --conf spark.dynamicAllocation.maxExecutors=${spark_max_executors} --driver-memory ${spark_driver_memory} --queue ${queue_name}</spark-opts>
            <arg>--cassandra_host</arg>
            <arg>${cassandra3_host}</arg>
            <arg>--cassandra_username</arg>
            <arg>${cassandra3_username}</arg>
            <arg>--cassandra_password</arg>
            <arg>${cassandra3_password}</arg>
            <arg>--cassandra_keyspace</arg>
            <arg>${cassandra_keyspace}</arg>
            <arg>--cassandra_table</arg>
            <arg>${cassandra_table}</arg>
            <arg>--cassandra_load_parallelism</arg>
            <arg>${cassandra_parallel_loaders}</arg>
            <arg>--cassandra_columns</arg>
            <arg>${cassandra3_columns}</arg>
            <arg>--hql_query</arg>
            <arg>${replaceAll(replaceAll(replaceAll(replaceAll(cassandra3_hql, "_YEAR_", year), "_MONTH_", month), "_DAY_", day), "_HOUR_", hour)}</arg>
        </spark>
        <ok to="end" />
        <error to="send_error_email" />
    </action>

    <action name="send_error_email">
        <sub-workflow>
            <app-path>${send_error_email_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>parent_name</name>
                    <value>${wf:name()}</value>
                </property>
                <property>
                    <name>parent_failed_action</name>
                    <value>${wf:lastErrorNode()}</value>
                </property>
                <property>
                    <name>parent_error_code</name>
                    <value>${wf:errorCode(wf:lastErrorNode())}</value>
                </property>
                <property>
                    <name>parent_error_message</name>
                    <value>${wf:errorMessage(wf:lastErrorNode())}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="kill"/>
        <error to="kill"/>
    </action>

    <kill name="kill">
        <message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name="end"/>
</workflow-app>
