<?xml version="1.0" encoding="UTF-8"?>
<workflow-app xmlns="uri:oozie:workflow:0.4"
    name="cassandra-monthly-wf-${cassandra_keyspace}-${year}-${month}">

    <parameters>

        <!-- Default values for inner oozie settings -->
        <property>
            <name>oozie_launcher_queue_name</name>
            <value>${queue_name}</value>
        </property>
        <property>
            <name>oozie_launcher_memory</name>
            <value>2048</value>
        </property>

        <!-- Required properties -->
        <property><name>queue_name</name></property>
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>

        <property>
            <name>hive_script</name>
            <description>Hive script to run.</description>
        </property>
        <property>
            <name>hive_site_xml</name>
            <description>hive-site.xml file path in HDFS</description>
        </property>
        <property>
            <name>source_table</name>
            <description>Hive table to use</description>
        </property>
        <property>
            <name>source_table_per_domain</name>
            <description>Hive table to take per domain uniques from</description>
        </property>
        <property>
            <name>source_table_per_project_family</name>
            <description>Hive table to take per project family uniques from</description>
        </property>
        <property>
            <name>temporary_directory</name>
            <description>A temporary directory to store data.</description>
        </property>
        <property>
            <name>send_error_email_workflow_file</name>
            <description>Workflow for sending an email</description>
        </property>

        <!-- cassandra loader -->
        <property>
            <name>refinery_cassandra_jar_path</name>
            <description>The refinery-cassandra jar file path in HDFS</description>
        </property>
        <property>
            <name>cassandra_reducer_class</name>
            <description>The reducer class to be used in refinery-cassandra jar</description>
        </property>
        <property>
            <name>cassandra_output_format_class</name>
            <description>The output format class to be used in refinery-cassandra jar</description>
        </property>
        <property>
            <name>cassandra_parallel_loaders</name>
            <description>The number of reducers to parallel load cassandra</description>
        </property>
        <property>
            <name>cassandra_nodes</name>
            <description>The number of nodes of the cassandra cluster</description>
        </property>
        <property>
            <name>batch_size</name>
            <description>The size of a batch sent to a node for insertion</description>
        </property>

        <!-- cassandra settings -->
        <property>
            <name>cassandra_host</name>
            <description>The cassandra host to connect for load</description>
        </property>
        <property>
            <name>cassandra_username</name>
            <description>The cassandra username for load</description>
        </property>
        <property>
            <name>cassandra_password</name>
            <description>The cassandra password for load</description>
        </property>
        <property>
            <name>cassandra_write_consistency</name>
            <description>The cassandra write consistency level</description>
        </property>

        <!-- Cassandra load job parameters-->
        <property>
            <name>hive_fields</name>
            <description>Hive output fields names</description>
        </property>
        <property>
            <name>hive_fields_types</name>
            <description>Hive output fields types</description>
        </property>

        <property>
            <name>cassandra_keyspace</name>
            <description>Keyspace to load</description>
        </property>
        <property>
            <name>cassandra_table</name>
            <description>Table to load</description>
        </property>
        <property>
            <name>cassandra_fields</name>
            <description>Fields from hive_fields to be loaded</description>
        </property>
        <property>
            <name>cassandra_primary_keys</name>
            <description>Primary keys name in cassandra</description>
        </property>

        <property>
            <name>constant_output_domain_value</name>
            <description>Constant value for cassandra _domain field</description>
        </property>
        <property>
            <name>constant_output_granularity_value</name>
            <description>Constant value for cassandra granularity field</description>
        </property>
        <property>
            <name>constant_output_tid_value</name>
            <description>Constant value for cassandra _tid field</description>
        </property>

        <!-- Time parameters-->
        <property>
            <name>year</name>
            <description>The partition's year</description>
        </property>
        <property>
            <name>month</name>
            <description>The partition's month</description>
        </property>

    </parameters>

    <start to="prepare_data"/>

    <action name="prepare_data">
        <hive xmlns="uri:oozie:hive-action:0.2">
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <job-xml>${hive_site_xml}</job-xml>
            <configuration>
                <property>
                    <name>mapreduce.job.queuename</name>
                    <value>${queue_name}</value>
                </property>
                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>
                <!--Let hive decide on the number of reducers -->
                <property>
                    <name>mapred.reduce.tasks</name>
                    <value>-1</value>
                </property>
                <property>
                    <name>hive.exec.scratchdir</name>
                    <value>/tmp/hive-${user}</value>
                </property>
            </configuration>

            <script>${hive_script}</script>
            <param>source_table=${source_table}</param>
            <!--
            The following two properties are only required by uniques. We're
            adding a default value so that the coordinator doesn't fail for
            the rest of the jobs, which don't use them.
            -->
            <param>source_table_per_domain=${source_table_per_domain}</param>
            <param>source_table_per_project_family=${source_table_per_project_family}</param>

            <param>separator=${hive_value_separator}</param>
            <param>refinery_hive_jar_path=${refinery_hive_jar_path}</param>
            <param>destination_directory=${temporary_directory}/${wf:id()}-${cassandra_keyspace}</param>
            <param>year=${year}</param>
            <param>month=${month}</param>
        </hive>

        <ok to="load_cassandra"/>
        <error to="send_error_email"/>
    </action>

    <action name="load_cassandra">
                <map-reduce>
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <configuration>

                <!-- Global Configuration -->

                <!-- Ensure classpath jars are loading in correct order-->
                <property>
                    <name>mapreduce.job.user.classpath.first</name>
                    <value>true</value>
                </property>

                <!--Set queue -->
                <property>
                    <name>mapreduce.job.queuename</name>
                    <value>${queue_name}</value>
                </property>

                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>

                <!-- Set mapper and reducer to yarn api style-->
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>

                <!-- Set logging to INFO -->
                <property>
                    <name>mapreduce.map.log.level</name>
                    <value>INFO</value>
                </property>
                <property>
                    <name>mapreduce.reduce.log.level</name>
                    <value>INFO</value>
                </property>



                <!-- Map side -->
                <!-- Default mapper - Nothing done -->
                <property>
                    <name>mapreduce.map.class</name>
                    <value>org.apache.hadoop.mapreduce.Mapper</value>
                </property>
                <!-- compress map output -->
                <property>
                    <name>mapreduce.map.output.compress</name>
                    <value>true</value>
                </property>
                <!-- input format and classes -->
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.TextInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.input.key.class</name>
                    <value>org.apache.hadoop.io.LongWritable</value>
                </property>
                <property>
                    <name>mapreduce.job.input.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <!-- map output classes-->
                <property>
                    <name>mapreduce.map.output.key.class</name>
                    <value>org.apache.hadoop.io.LongWritable</value>
                </property>
                <property>
                    <name>mapreduce.map.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <!-- input dir -->
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>${temporary_directory}/${wf:id()}-${cassandra_keyspace}</value>
                </property>


                <!-- Reduce side -->
                <!-- Load to Cassandra reducer with X reducers-->
                <property>
                    <name>mapreduce.reduce.class</name>
                    <value>${cassandra_reducer_class}</value>
                </property>
                <property>
                    <name>mapreduce.job.reduces</name>
                    <value>${cassandra_parallel_loaders}</value>
                </property>
                <property>
                    <name>mapreduce.reduce.speculative</name>
                    <value>false</value>
                </property>

                <!-- Cassandra output format and classes -->
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>${cassandra_output_format_class}</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>java.util.Map</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>java.util.List</value>
                </property>

                <!-- Cassandra reducer core config-->
                <!-- Cassandra host - legacy naming-->
                <property>
                    <name>cassandra.output.thrift.address</name>
                    <value>${cassandra_host}</value>
                </property>
                <property>
                    <name>cassandra.output.native.port</name>
                    <value>${cassandra_port}</value>
                </property>
                <property>
                    <name>cassandra.input.native.auth.provider</name>
                    <value>com.datastax.driver.core.PlainTextAuthProvider</value>
                </property>
                <property>
                    <name>cassandra.username</name>
                    <value>${cassandra_username}</value>
                </property>
                <property>
                    <name>cassandra.password</name>
                    <value>${cassandra_password}</value>
                </property>
                <property>
                    <!--Write consistency level -->
                    <name>cassandra.consistencylevel.write</name>
                    <value>${cassandra_write_consistency}</value>
                </property>
                <property>
                    <!--Quoted on purpose -->
                    <name>cassandra.output.keyspace</name>
                    <value>"${cassandra_keyspace}"</value>
                </property>
                <property>
                    <!-- actually used for column familly - quoted on purpose-->
                    <name>mapreduce.output.basename</name>
                    <value>"${cassandra_table}"</value>
                </property>
                <property>
                    <name>cassandra.output.partitioner.class</name>
                    <value>Murmur3Partitioner</value>
                </property>
                <property>
                    <name>cassandra.output.cql</name>
                    <value>${cassandra_cql}</value>
                </property>
                <property>
                    <name>mapreduce.output.columnfamilyoutputformat.batch.threshold</name>
                    <value>${batch_size}</value>
                </property>
                <property>
                    <name>mapreduce.output.columnfamilyoutputformat.queue.size</name>
                    <value>${batch_size * cassandra_nodes}</value>
                </property>


                <!-- Cassandra reducer specific config-->
                <property>
                    <name>input_separator</name>
                    <value>${hive_value_separator}</value>
                </property>
                <property>
                    <name>input_fields</name>
                    <value>${hive_fields}</value>
                </property>
                <property>
                    <name>input_fields_types</name>
                    <value>${hive_fields_types}</value>
                </property>
                <property>
                    <name>output_fields</name>
                    <value>${cassandra_fields}</value>
                </property>
                <property>
                    <name>output_primary_keys</name>
                    <value>${cassandra_primary_keys}</value>
                </property>
                <property>
                    <name>${constant_output_domain_field}</name>
                    <value>${constant_output_domain_value}</value>
                </property>
                <property>
                    <name>${constant_output_granularity_field}</name>
                    <value>${constant_output_granularity_value}</value>
                </property>
                <property>
                    <name>${constant_output_tid_field}</name>
                    <value>${constant_output_tid_value}</value>
                </property>
            </configuration>
            <archive>${refinery_cassandra_jar_path}</archive>

        </map-reduce>
        <ok to="remove_temporary_data"/>
        <error to="send_error_email"/>
    </action>

    <action name="remove_temporary_data">
        <fs>
            <delete path="${temporary_directory}/${wf:id()}-${cassandra_keyspace}"/>
        </fs>
        <ok to="end"/>
        <error to="send_error_email"/>
    </action>

    <action name="send_error_email">
        <sub-workflow>
            <app-path>${send_error_email_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>parent_name</name>
                    <value>${wf:name()}</value>
                </property>
                <property>
                    <name>parent_failed_action</name>
                    <value>${wf:lastErrorNode()}</value>
                </property>
                <property>
                    <name>parent_error_code</name>
                    <value>${wf:errorCode(wf:lastErrorNode())}</value>
                </property>
                <property>
                    <name>parent_error_message</name>
                    <value>${wf:errorMessage(wf:lastErrorNode())}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="kill"/>
        <error to="kill"/>
    </action>


    <kill name="kill">
        <message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name="end"/>
</workflow-app>
