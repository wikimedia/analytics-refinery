<?xml version="1.0" encoding="UTF-8"?>
<coordinator-app xmlns="uri:oozie:coordinator:0.4"
    xmlns:sla="uri:oozie:sla:0.2"
    name="cassandra-daily-coord-${cassandra_keyspace}"
    frequency="${coord:days(1)}"
    start="${start_time}"
    end="${stop_time}"
    timezone="Universal">

    <parameters>

        <!-- Required properties -->
        <property><name>queue_name</name></property>
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive2_jdbc_url</name></property>
        <property><name>workflow_file</name></property>

        <property><name>start_time</name></property>
        <property><name>stop_time</name></property>

        <property><name>datasets_file</name></property>
        <property><name>dataset_name</name></property>
        <property><name>dataset_freq</name></property>

        <property><name>hive_site_xml</name></property>
        <property><name>hive_script</name></property>
        <property><name>hive_value_separator</name></property>
        <property><name>source_table</name></property>
        <property><name>temporary_directory</name></property>
        <property><name>refinery_hive_jar_path</name></property>

        <property><name>refinery_cassandra_jar_path</name></property>
        <property><name>cassandra_reducer_class</name></property>
        <property><name>cassandra_output_format_class</name></property>
        <property><name>cassandra_parallel_loaders</name></property>
        <property><name>cassandra_nodes</name></property>
        <property><name>batch_size</name></property>

        <property><name>cassandra_host</name></property>
        <property><name>cassandra_username</name></property>
        <property><name>cassandra_password</name></property>
        <property><name>cassandra_write_consistency</name></property>

        <property><name>hive_fields</name></property>
        <property><name>hive_fields_types</name></property>

        <property><name>cassandra_keyspace</name></property>
        <property><name>cassandra_table</name></property>
        <property><name>cassandra_fields</name></property>
        <property><name>cassandra_primary_keys</name></property>

        <property><name>constant_output_domain_field</name></property>
        <property><name>constant_output_domain_value</name></property>
        <property><name>constant_output_granularity_field</name></property>
        <property><name>constant_output_tid_field</name></property>
        <property><name>constant_output_tid_value</name></property>

        <property><name>send_error_email_workflow_file</name></property>
        <property><name>sla_alert_contact</name></property>

        <!-- cassandra3 loading-->
        <property><name>hive_metastore_uri</name></property>
        <property><name>oozie_spark_lib</name></property>
        <property><name>spark_master</name></property>
        <property><name>spark_deploy</name></property>
        <property><name>spark_job_name</name></property>
        <property><name>spark_job_class</name></property>
        <property><name>spark_job_jar</name></property>
        <property><name>spark_assembly_zip</name></property>
        <property><name>spark_executor_memory</name></property>
        <property><name>spark_executor_cores</name></property>
        <property><name>spark_driver_memory</name></property>
        <property><name>spark_max_executors</name></property>
        <property><name>cassandra3_host</name></property>
        <property><name>cassandra3_username</name></property>
        <property><name>cassandra3_password</name></property>
        <property><name>cassandra3_columns</name></property>
        <property><name>cassandra3_hql</name></property>

        <!--
        The following properties are only required by uniques. We're adding a
        default value so that the coordinator doesn't fail for the rest of the
        jobs, which don't use them.
        -->
        <property>
            <name>source_table_per_domain</name>
            <value>wmf.unique_devices_per_domain_daily</value>
        </property>
        <property>
            <name>source_table_per_project_family</name>
            <value>wmf.unique_devices_per_project_family_daily</value>
        </property>

        <!--
        The following property is only required by certain country-wise jobs.
        We're adding a default value so that the coordinator doesn't fail for
        the rest of the jobs, which don't use them.
        -->
        <property>
            <name>country_blacklist_table</name>
            <value>wmf.geoeditors_blacklist_country</value>
        </property>

        <!--
            The following property is usefull to delay jobs, allowing to wait
            for more of the needed files to be available before starting.
            Default delay is set to 0 and can be overwritten in each job
            conf.
            Note: The actual delay depends on the dataset-frequency as we specify
            a number, and the unit of time this number applies is the
            dataset-frequency

        -->
        <property>
            <name>delay</name>
            <value>0</value>
        </property>

    </parameters>

    <controls>
        <!--
        By having materialized jobs not timeout, we ease backfilling incidents
        after recoverable hiccups on the dataset producers.
        -->
        <timeout>-1</timeout>

        <concurrency>1</concurrency>

        <throttle>2</throttle>
    </controls>

    <datasets>
        <!--
        Include pageview or projectview hourly dataset files.
        $datasets_file will be used as the input events.
        -->
        <include>${datasets_file}</include>
    </datasets>

    <input-events>
        <data-in name="data_input" dataset="${dataset_name}">
            <start-instance>${coord:current(0)}</start-instance>
            <end-instance>${(dataset_freq == "day") ? coord:current(0) : coord:current(23)}</end-instance>
        </data-in>
        <data-in name="data_input_delay" dataset="${dataset_name}">
            <instance>${(dataset_freq == "day") ? coord:current(0 + delay) : coord:current(23 + delay)}</instance>
        </data-in>
    </input-events>

    <action>
        <workflow>
            <app-path>${workflow_file}</app-path>
            <configuration>
                <!-- new properties to be passed to the workflow -->

                <property>
                    <name>constant_output_granularity_value</name>
                    <value>daily,text</value>
                </property>

                <property>
                    <name>year</name>
                    <value>${coord:formatTime(coord:nominalTime(), "y")}</value>
                </property>
                <property>
                    <name>month</name>
                    <value>${coord:formatTime(coord:nominalTime(), "M")}</value>
                </property>
                <property>
                    <name>day</name>
                    <value>${coord:formatTime(coord:nominalTime(), "d")}</value>
                </property>

            </configuration>
        </workflow>

        <sla:info>
            <!--
                Use action actual time as SLA base, since it's the time used
                to compute timeout
                We use 1 day and a half (32 hours) as the dataset is generic
                and needs a full day of data
            -->
            <sla:nominal-time>${coord:actualTime()}</sla:nominal-time>
            <sla:should-end>${32 * HOURS}</sla:should-end>
            <sla:alert-events>end_miss</sla:alert-events>
            <sla:alert-contact>${sla_alert_contact}</sla:alert-contact>
        </sla:info>

    </action>
</coordinator-app>
