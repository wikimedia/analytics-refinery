# Configures a coordinator to load editors by country to cassandra.
# Any of the following properties are overidable with -D.
#
# Usage (production):
# sudo -u analytics kerberos-run-command analytics oozie job --oozie $OOZIE_URL \
#   -Drefinery_directory=hdfs://analytics-hadoop$(hdfs dfs -ls -d /wmf/refinery/$(date +"%Y")* | tail -n 1 | awk '{print $NF}') \
#   -Dqueue='production' \
#   -Dstart_time='2020-08-01T00:00Z' \
#   -config oozie/cassandra/coord_editors_bycountry_monthly.properties \
#   -run

# Main Hadoop properties.
name_node                         = hdfs://analytics-hadoop
job_tracker                       = resourcemanager.analytics.eqiad.wmnet:8032
hive_principal                    = hive/analytics-hive.eqiad.wmnet@WIKIMEDIA
hive2_jdbc_url                    = jdbc:hive2://analytics-hive.eqiad.wmnet:10000/default
queue_name                        = default
user                              = analytics
hive_site_xml                     = ${name_node}/user/hive/hive-site.xml
temporary_directory               = ${name_node}/wmf/tmp/analytics

# Base path in HDFS to refinery. When submitting this job for production,
# you should override this as shown in the usage example above.
refinery_directory                = ${name_node}/wmf/refinery/current

# HDFS path to artifacts that will be used by this job.
# E.g. refinery-hive.jar should exist here.
artifacts_directory               = ${refinery_directory}/artifacts

# Base path in HDFS to oozie files.
oozie_directory                   = ${refinery_directory}/oozie

# Jar path for the Cassandra loader.
refinery_cassandra_jar_path       = ${artifacts_directory}/org/wikimedia/analytics/refinery/refinery-cassandra-0.1.1.jar
cassandra_reducer_class           = org.wikimedia.analytics.refinery.cassandra.ReducerToCassandra
cassandra_output_format_class     = org.wikimedia.analytics.refinery.cassandra.CqlOutputFormat

# Jar path for the UDF to generate country names.
refinery_hive_jar_path            = ${artifacts_directory}/org/wikimedia/analytics/refinery/refinery-hive-0.1.1.jar

# HDFS paths to oozie xml files.
coordinator_file                  = ${oozie_directory}/cassandra/monthly/coordinator.xml
workflow_file                     = ${oozie_directory}/cassandra/monthly/workflow.xml
datasets_file                     = ${oozie_directory}/mediawiki/geoeditors/datasets.xml
send_error_email_workflow_file    = ${oozie_directory}/util/send_error_email/workflow.xml

# Path used by external datasets file.
mw_private_directory              = ${name_node}/wmf/data/wmf/mediawiki_private

# Provides name and frequency of the dataset.
# Used to manage different datasets correctly in coordinator.
dataset_name                      = geoeditors_public_monthly
dataset_freq                      = month

# Initial import time of the geoeditors dataset.
start_time                        = 2018-01-01T00:00Z
# Time to stop running this coordinator.  Year 3000 == never!
stop_time                         = 3000-01-01T00:00Z

# Source data properties.
hive_script                       = editors_bycountry.hql
source_table                      = wmf.geoeditors_public_monthly
hive_value_separator              = \\t
hive_fields                       = project,activity-level,year,month,countriesJSON
hive_fields_types                 = text,text,text,text,text

# Cassandra properties.
cassandra_host                    = aqs1004-a.eqiad.wmnet
cassandra_port                    = 9042
cassandra_username                = aqsloader
cassandra_password                = cassandra
cassandra_nodes                   = 6
batch_size                        = 1024
cassandra_write_consistency       = LOCAL_QUORUM
cassandra_parallel_loaders        = 1
cassandra_keyspace                = local_group_default_T_editors_bycountry
cassandra_table                   = data
cassandra_cql                     = UPDATE "${cassandra_keyspace}"."data" SET "countriesJSON" = ?
cassandra_fields                  = countriesJSON
cassandra_primary_keys            = _domain,project,activity-level,year,month,_tid

# Constant field names and values to be loaded into cassandra.
constant_output_domain_field      = _domain
constant_output_domain_value      = analytics.wikimedia.org,text
constant_output_granularity_field = granularity
constant_output_tid_field         = _tid
constant_output_tid_value         = 0,timeuuid

# Cassandra3 loading with spark
hive_metastore_uri                = thrift://analytics-hive.eqiad.wmnet:9083
oozie_spark_lib                   = spark-2.4.4
spark_master                      = yarn
spark_deploy                      = cluster
spark_job_name                    = cassandra3-local_group_default_T_editors_bycountry-monthly
spark_job_class                   = org.wikimedia.analytics.refinery.job.HiveToCassandra
spark_job_jar                     = ${artifacts_directory}/org/wikimedia/analytics/refinery/refinery-job-0.1.17.jar
spark_assembly_zip                = ${name_node}/user/spark/share/lib/spark2-assembly.zip
spark_executor_memory             = 8G
spark_executor_cores              = 4
spark_driver_memory               = 4G
spark_max_executors               = 32
cassandra3_host                   = aqs1010-a.eqiad.wmnet:9042
cassandra3_username               = aqsloader
cassandra3_password               = cassandra
cassandra3_columns                = _domain,project,activity-level,year,month,_tid,countriesJSON

################################## BEGINNING OF QUERY
# Using \ as last character makes the property being multi-line (beware of trailing spaces!)
# Using _YEAR_ and _MONTH_ values to be replaced by actual values
# Need to escape \ char 4 times (!!!) due to the multiple systems the string passes through
#
# Note: Can't add comments in the query, adding them here
#  - DISTRIBUTE BY and SORT BY is secondary sorting in Hive:
#      It uses SORT BY, not ORDER BY, to sort in each reducer instead of globally
#      The sorting-key is a superset of the grouping-key

cassandra3_hql                    = WITH prepared_editors_by_country AS ( \
    SELECT \
        project, \
        activity_level, \
        month, \
        editors_ceil, \
        country_code \
    FROM wmf.geoeditors_public_monthly \
    WHERE \
        month = CONCAT(LPAD('_YEAR_', 4, '0'),'-', LPAD('_MONTH_', 2, '0')) \
    DISTRIBUTE BY \
        project, \
        activity_level, \
        month \
    SORT BY \
        project, \
        activity_level, \
        month, \
        editors_ceil DESC, \
        country_code \
) \
SELECT \
    'analytics.wikimedia.org' as _domain, \
    project, \
    CASE \
        WHEN activity_level = '5 to 99' THEN '5..99-edits' \
        WHEN activity_level = '100 or more' THEN '100..-edits' \
        ELSE NULL \
    END as `activity-level`, \
    LPAD('_YEAR_', 4, '0') as year, \
    LPAD('_MONTH_', 2, '0') as month, \
    '13814000-1dd2-11b2-8080-808080808080' as _tid, \
    CONCAT('[', \
        CONCAT_WS( \
            ',', \
            COLLECT_LIST( \
                CONCAT( \
                    '{"country":"', country_code, \
                    '","editors-ceil":', CAST(editors_ceil AS STRING), '}' \
                ) \
            ) \
        ), \
    ']') as countriesJSON \
FROM prepared_editors_by_country \
GROUP BY \
    project, \
    activity_level, \
    month

######################### END OF QUERY


# SLA email to make sure we receive email if the job times out.
sla_alert_contact                 = analytics-alerts@wikimedia.org

# Coordintator to start.
oozie.coord.application.path      = ${coordinator_file}
oozie.use.system.libpath          = true
oozie.action.external.stats.write = true
