# Configures a bundle to manage automatically adding Hive partitions to
# a table populated from MediaWiki via Kafka & Avro.
# Any of the following properties are overidable with -D.
# Usage:
# oozie job -submit -config oozie/mediawiki/load/bundle.properties.
#
# NOTE:  The $oozie_directory must be synced to HDFS so that all relevant
#        .xml files exist there when this job is submitted.


name_node                         = hdfs://analytics-hadoop
job_tracker                       = resourcemanager.analytics.eqiad.wmnet:8032
queue_name                        = default

# In T220971 we moved all the oozie bundle/coordinators to the analytics user,
# except this one, since it should be deprecated when MediaWiki Avro / Monolog
# is migrated to Event Gate.
user                              = hdfs

# Base path in HDFS to refinery.
# When submitting this job for production, you should
# override this to point directly at a deployed
# directory name, and not the 'symbolic' 'current' directory.
# E.g.  /wmf/refinery/2015-01-05T17.59.18Z--7bb7f07
refinery_directory                = ${name_node}/wmf/refinery/current

# Base path in HDFS to oozie files.
# Other files will be used relative to this path.
oozie_directory                   = ${refinery_directory}/oozie

# HDFS path to coordinator to run for each webrequest_source.
coordinator_file                  = ${oozie_directory}/mediawiki/load/coordinator.xml

# HDFS path to workflow to run.
workflow_file                     = ${oozie_directory}/mediawiki/load/workflow.xml

# HDFS path to datasets definition
datasets_raw_file                 = ${oozie_directory}/mediawiki/datasets_raw.xml

# Initial import time of the dataset.
start_time                        = 2015-11-01T00:00Z

# Time to stop running this coordinator.  Year 3000 == never!
stop_time                         = 3000-01-01T00:00Z

# HDFS path to hive-site.xml file.  This is needed to run hive actions.
hive_site_xml                     = ${name_node}/user/hive/hive-site.xml

# Hive database for output
database                          = wmf_raw

# Coodinators expect to find datasets in $raw_base_directory/mediawiki_$channel
raw_base_directory                = ${name_node}/wmf/data/raw/mediawiki

# Common email addresses to send workflow errors to.
# Each coordinator can add additional email addresses to notify in bundle.xml
common_error_to                   = analytics-alerts@wikimedia.org

# Coordinator to start.
oozie.bundle.application.path     = ${oozie_directory}/mediawiki/load/bundle.xml
oozie.use.system.libpath          = true
oozie.action.external.stats.write = true
