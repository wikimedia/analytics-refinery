<?xml version="1.0" encoding="UTF-8"?>
<coordinator-app xmlns="uri:oozie:coordinator:0.4"
    xmlns:sla="uri:oozie:sla:0.2"
    name="mediawiki-history-denormalize-coord"
    frequency="${coord:months(1)}"
    start="${start_time}"
    end="${stop_time}"
    timezone="Universal">

    <parameters>
        <!-- Required properties -->
        <property><name>queue_name</name></property>
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive_metastore_uri</name></property>
        <property><name>workflow_file</name></property>
        <property><name>start_time</name></property>
        <property><name>stop_time</name></property>

        <property><name>datasets_raw_file</name></property>
        <property><name>mw_raw_directory</name></property>
        <property><name>datasets_raw_private_file</name></property>
        <property><name>mw_raw_private_directory</name></property>
        <property><name>datasets_file</name></property>
        <property><name>mw_directory</name></property>

        <property><name>oozie_spark_lib</name></property>
        <property><name>spark_master</name></property>
        <property><name>spark_assembly_zip</name></property>
        <property><name>spark_job_jar</name></property>
        <property><name>spark_job_class</name></property>
        <property><name>spark_executor_memory</name></property>
        <property><name>spark_executor_cores</name></property>
        <property><name>spark_executor_memory_overhead</name></property>
        <property><name>spark_driver_memory</name></property>
        <property><name>spark_max_executors</name></property>

        <property><name>spark_partitions_number</name></property>
        <property><name>tmp_path</name></property>

        <property><name>send_error_email_workflow_file</name></property>

        <property><name>sla_alert_contact</name></property>
    </parameters>

    <controls>
        <!--(timeout is measured in minutes)-->
        <timeout>-1</timeout>

        <!-- Setting low concurrency for resource sharing.
             The job runs pretty fast (~1 minute) and increasing concurrency should not cause any problems-->
        <concurrency>1</concurrency>

        <throttle>2</throttle>

    </controls>

    <datasets>
        <!--
        Include raw, raw_private and history datasets files.
        -->
        <include>${datasets_raw_file}</include>
        <include>${datasets_raw_private_file}</include>
        <include>${datasets_file}</include>
    </datasets>

    <!-- Reminder: This job doesn't depend on hive partitions being created
         over its data since it reads files. -->
    <input-events>
        <data-in name="mw_project_namespace_map" dataset="mw_project_namespace_map">
            <instance>${coord:current(0)}</instance>
        </data-in>
        <data-in name="mw_archive_table" dataset="mw_archive_table">
            <instance>${coord:current(0)}</instance>
        </data-in>
        <data-in name="mw_change_tag_table" dataset="mw_change_tag_table">
            <instance>${coord:current(0)}</instance>
        </data-in>
        <data-in name="mw_change_tag_def_table" dataset="mw_change_tag_def_table">
            <instance>${coord:current(0)}</instance>
        </data-in>
        <data-in name="mw_logging_table" dataset="mw_logging_table">
            <instance>${coord:current(0)}</instance>
        </data-in>
        <data-in name="mw_page_table" dataset="mw_page_table">
            <instance>${coord:current(0)}</instance>
        </data-in>
        <data-in name="mw_revision_table" dataset="mw_revision_table">
            <instance>${coord:current(0)}</instance>
        </data-in>
        <data-in name="mw_user_table" dataset="mw_user_table">
            <instance>${coord:current(0)}</instance>
        </data-in>
        <data-in name="mw_user_groups_table" dataset="mw_user_groups_table">
            <instance>${coord:current(0)}</instance>
        </data-in>
        <!-- Private datasets -->
        <data-in name="mw_private_actor_table" dataset="mw_private_actor_table">
            <instance>${coord:current(0)}</instance>
        </data-in>
        <data-in name="mw_private_comment_table" dataset="mw_private_comment_table">
            <instance>${coord:current(0)}</instance>
        </data-in>
    </input-events>

    <output-events>
        <!-- Datasets for data synchronisation-->
        <data-out name="mw_user_history_unchecked" dataset="mw_user_history_unchecked">
            <instance>${coord:current(0)}</instance>
        </data-out>
        <data-out name="mw_page_history_unchecked" dataset="mw_page_history_unchecked">
            <instance>${coord:current(0)}</instance>
        </data-out>
        <data-out name="mw_denormalized_history_unchecked" dataset="mw_denormalized_history_unchecked">
            <instance>${coord:current(0)}</instance>
        </data-out>
    </output-events>

    <action>
        <workflow>
            <app-path>${workflow_file}</app-path>
            <configuration>
                <property>
                    <name>snapshot</name>
                    <value>${coord:formatTime(coord:nominalTime(), "yyyy")}-${coord:formatTime(coord:nominalTime(), "MM")}</value>
                </property>
            </configuration>
        </workflow>

        <sla:info>
            <!--
                Use action actual time as SLA base, since it's the time used
                to compute timeout
                Job is waiting for the month data to be present, then sqoop
                does its job, so waiting for 3 days after month end should
                be enough.
            -->
            <sla:nominal-time>${coord:actualTime()}</sla:nominal-time>
            <sla:should-end>${34 * DAYS}</sla:should-end>
            <sla:alert-events>end_miss</sla:alert-events>
            <sla:alert-contact>${sla_alert_contact}</sla:alert-contact>
        </sla:info>

    </action>
</coordinator-app>
