<?xml version="1.0" encoding="UTF-8"?>
<workflow-app xmlns="uri:oozie:workflow:0.4"
    name="mediawiki-history-reduced-wf-${new_snapshot}">

    <parameters>

        <!-- Allows changing spark version to be used by oozie -->
        <property>
            <name>oozie.action.sharelib.for.spark</name>
            <value>${oozie_spark_lib}</value>
        </property>

        <!-- Default values for inner oozie settings -->
        <property>
            <name>oozie_launcher_queue_name</name>
            <value>${queue_name}</value>
        </property>
        <property>
            <name>oozie_launcher_memory</name>
            <value>2048</value>
        </property>

        <!-- Required properties -->
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>queue_name</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive2_jdbc_url</name></property>
        <property><name>hive_metastore_uri</name></property>

        <property>
            <name>hive_site_xml</name>
            <description>hive-site.xml file path in HDFS</description>
        </property>
        <property>
            <name>spark_master</name>
            <description>Master to be used for Spark (yarn, local, other)</description>
        </property>
        <property>
            <name>spark_assembly_zip</name>
            <description>The spark assembly zip file on HDFS, preventing to repackage it everytime</description>
        </property>
        <property>
            <name>spark_job_jar</name>
            <description>Path to the jar to be used to run spark job</description>
        </property>
        <property>
            <name>spark_job_class</name>
            <description>Class of the spark job to be run</description>
        </property>
        <property>
            <name>spark_executor_memory</name>
            <description>Memory to allocate for each spark executor</description>
        </property>
        <property>
            <name>spark_executor_cores</name>
            <description>Number of cores to allocate for each spark executor</description>
        </property>
        <property>
            <name>spark_executor_memory_overhead</name>
            <description>Overhead of memory for spark executor to work</description>
        </property>
        <property>
            <name>spark_driver_memory</name>
            <description>Memory to allocate for spark driver process</description>
        </property>
        <property>
            <name>spark_max_num_executors</name>
            <description>Maximum number of concurrent executors for spark with dynamic allocation</description>
        </property>
        <property>
            <name>spark_partitions_number</name>
            <description>Number of partitions to be used by spark job to parallelize tasks</description>
        </property>

        <property>
            <name>mw_denormalized_history_table</name>
            <description>Mediawiki denormalized history table to get data from</description>
        </property>
        <property>
            <name>mw_project_namespace_map_table</name>
            <description>Mediawiki project_namespace_map table to use</description>
        </property>
        <property>
            <name>mw_history_reduced_table</name>
            <description>Mediawiki denormalized history table to get data from</description>
        </property>
        <property>
            <name>previous_snapshot</name>
            <description>The previous snapshot used to check validity of the new one (YYYY-MM)</description>
        </property>
        <property>
            <name>new_snapshot</name>
            <description>The new snapshot to reduce, check and index (YYYY-MM)</description>
        </property>

        <property>
            <name>wikis_to_check</name>
            <description>The number of wikis to use in the check (by decreasing edit activity)</description>
        </property>
        <property>
            <name>min_events_growth</name>
            <description>The minimum value for which events-growth don't fail</description>
        </property>
        <property>
            <name>max_events_growth</name>
            <description>The maximum value for which events-growth don't fail</description>
        </property>
        <property>
            <name>wrongs_rows_ratio</name>
            <description>The maximum value for which errors-rows-ratio don't fail</description>
        </property>

        <property>
            <name>mw_directory</name>
            <description>Path to mediawiki processed data on Hadoop</description>
        </property>
        <property>
            <name>history_reduced_location</name>
            <description>The folder where the mediawiki_history_reduced partition is written</description>
        </property>
        <property>
            <name>check_errors_folder</name>
            <value>${mw_directory}/history_reduced_check_errors/snapshot=${new_snapshot}</value>
            <description>The folder where errors are written (if any)</description>
        </property>

        <property>
            <name>loaded_period</name>
            <description>Period of the data loaded in interval format (yyyy-MM-dd/yyyy-MM-dd)</description>
        </property>
        <property>
            <name>druid_template_file</name>
            <description>File to use as a template to define druid loading (absolute since used by load_druid sub-workflow)</description>
        </property>
        <property>
            <name>druid_overlord_url</name>
            <description>The druid overlord url used for loading</description>
        </property>
        <property>
            <name>druid_datasource_prefix</name>
            <description>The druid datasource prefix to be used, followed by snapshot value</description>
        </property>
        <property>
            <name>load_druid_workflow_file</name>
            <description>Workflow for loading druid</description>
        </property>
        <property>
            <name>mark_directory_done_workflow_file</name>
            <description>Workflow for marking a directory done</description>
        </property>
        <property>
            <name>send_error_email_workflow_file</name>
            <description>Workflow for sending an error email</description>
        </property>
        <property>
            <name>send_success_email_workflow_file</name>
            <description>Workflow for sending a success email</description>
        </property>
        <property>
            <name>success_email_to</name>
            <value>analytics-internal@wikimedia.org</value>
            <description>Comma-separated list of email addresses to send a success email to</description>
        </property>

    </parameters>

    <credentials>
        <credential name="hcat-cred" type="hcat">
            <property>
                <name>hcat.metastore.principal</name>
                <value>${hive_principal}</value>
            </property>
            <property>
               <name>hcat.metastore.uri</name>
               <value>${hive_metastore_uri}</value>
            </property>
        </credential>

        <credential name="hive2-cred" type="hive2">
            <property>
                <name>hive2.server.principal</name>
                <value>${hive_principal}</value>
            </property>
            <property>
               <name>hive2.jdbc.url</name>
               <value>${hive2_jdbc_url}</value>
            </property>
        </credential>
    </credentials>


    <start to="generate_mediawiki_history_reduced"/>

    <action name="generate_mediawiki_history_reduced" cred="hive2-cred">
        <hive2 xmlns="uri:oozie:hive2-action:0.2">
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <job-xml>${hive_site_xml}</job-xml>
            <configuration>
                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>

                <!--Let hive decide on the number of reducers -->
                <property>
                    <name>mapred.reduce.tasks</name>
                    <value>-1</value>
                </property>
                <property>
                    <name>hive.exec.scratchdir</name>
                    <value>/tmp/hive-${user}</value>
                </property>
            </configuration>
            <jdbc-url>${hive2_jdbc_url}</jdbc-url>
            <script>generate_mediawiki_history_reduced.hql</script>

            <!-- Query parameters -->
            <param>mw_denormalized_history_table=${mw_denormalized_history_table}</param>
            <param>mw_project_namespace_map_table=${mw_project_namespace_map_table}</param>
            <param>destination_table=${mw_history_reduced_table}</param>
            <param>snapshot=${new_snapshot}</param>

            <!-- Beeline arguments -->
            <argument>--verbose</argument>
            <argument>--hiveconf</argument>
            <argument>mapreduce.job.queuename=${queue_name}</argument>
        </hive2>
        <ok to="check_mediawiki_history_reduced" />
        <error to="send_error_email" />
    </action>


    <!-- Check and index reduced history -->
    <action name="check_mediawiki_history_reduced" cred="hcat-cred">
        <spark xmlns="uri:oozie:spark-action:0.1">
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <configuration>
                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>
            </configuration>
            <master>${spark_master}</master>
            <mode>${spark_deploy}</mode>
            <name>${spark_job_name}-${new_snapshot}</name>
            <class>${spark_job_class}</class>
            <jar>${spark_job_jar}</jar>
            <spark-opts>--conf spark.yarn.archive=${spark_assembly_zip} --executor-memory ${spark_executor_memory} --executor-cores ${spark_executor_cores} --driver-memory ${spark_driver_memory} --queue ${queue_name} --conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.dynamicAllocation.maxExecutors=${spark_max_num_executors} --conf spark.yarn.executor.memoryOverhead=${spark_executor_memory_overhead} --conf spark.yarn.maxAppAttempts=1</spark-opts>
            <arg>--mediawiki-history-base-path</arg>
            <arg>${mw_directory}</arg>
            <arg>--previous-snapshot</arg>
            <arg>${previous_snapshot}</arg>
            <arg>--new-snapshot</arg>
            <arg>${new_snapshot}</arg>
            <arg>--wikis-to-check</arg>
            <arg>${wikis_to_check}</arg>
            <arg>--num-partitions</arg>
            <arg>${spark_partitions_number}</arg>
            <arg>--min-events-growth-threshold</arg>
            <arg>${min_events_growth}</arg>
            <arg>--max-events-growth-threshold</arg>
            <arg>${max_events_growth}</arg>
            <arg>--wrong-rows-ratio-threshold</arg>
            <arg>${wrongs_rows_ratio}</arg>
            <arg>--check-reduced-history</arg>
        </spark>
        <ok to="check_mediawiki_history_reduced_result" />
        <error to="send_error_email" />
    </action>

    <decision name="check_mediawiki_history_reduced_result">
        <switch>
            <!-- Fail if errors folder exist, continue otherwise -->
            <case to="send_error_check_mediawiki_reduced_history_email">
                ${fs:exists(check_errors_folder) and fs:dirSize(check_errors_folder) gt 0}
            </case>
            <default to="mark_mediawiki_history_reduced_dataset_done"/>
        </switch>
    </decision>

    <action name="mark_mediawiki_history_reduced_dataset_done">
        <sub-workflow>
            <app-path>${mark_directory_done_workflow_file}</app-path>
            <configuration>
                <property>
                    <name>directory</name>
                    <value>${history_reduced_location}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="index_druid"/>
        <error to="send_error_email"/>
    </action>

    <action name="index_druid">
        <sub-workflow>
            <app-path>${load_druid_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>source_directory</name>
                    <value>${history_reduced_location}</value>
                </property>
                <property>
                    <name>template_file</name>
                    <value>${druid_template_file}</value>
                </property>
                <property>
                    <name>loaded_period</name>
                    <value>${loaded_period}</value>
                </property>
                <property>
                    <name>druid_overlord_url</name>
                    <value>${druid_overlord_url}</value>
                </property>
                <property>
                    <name>target_datasource</name>
                    <value>${druid_datasource_prefix}_${replaceAll(new_snapshot, '-', '_')}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="send_success_email"/>
        <error to="send_error_email"/>
    </action>

    <action name="send_success_email">
        <sub-workflow>
            <app-path>${send_success_email_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>parent_name</name>
                    <value>${wf:name()}</value>
                </property>
                <property>
                    <name>to</name>
                    <value>${success_email_to}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="end"/>
        <error to="send_error_email"/>
    </action>

<action name="send_error_check_mediawiki_reduced_history_email">
        <sub-workflow>
            <app-path>${send_error_email_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>subject</name>
                    <value>MediawikiHistoryReduced ERROR - Workflow ${wf:name()}</value>
                </property>
                <property>
                    <name>body</name>
                    <value>This is an ERROR.
Check of mediawiki-history-reduced snapshot ${new_snapshot} has generated a non-empty error file at ${check_errors_folder}.

Please, have a look at this error file attached to this email and take necessary action!

Thanks :)
-- Oozie</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="kill"/>
        <error to="send_error_email"/>
    </action>

    <action name="send_error_email">
        <sub-workflow>
            <app-path>${send_error_email_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>parent_name</name>
                    <value>${wf:name()}</value>
                </property>
                <property>
                    <name>parent_failed_action</name>
                    <value>${wf:lastErrorNode()}</value>
                </property>
                <property>
                    <name>parent_error_code</name>
                    <value>${wf:errorCode(wf:lastErrorNode())}</value>
                </property>
                <property>
                    <name>parent_error_message</name>
                    <value>${wf:errorMessage(wf:lastErrorNode())}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="kill"/>
        <error to="kill"/>
    </action>

    <kill name="kill">
        <message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name="end"/>
</workflow-app>
