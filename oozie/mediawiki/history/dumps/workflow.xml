<?xml version="1.0" encoding="UTF-8"?>

<workflow-app xmlns="uri:oozie:workflow:0.4"
    name="mediawiki-history-dumps-wf-${year}-${month}">

    <parameters>
        <property><name>year</name></property>
        <property><name>month</name></property>
        <property><name>job_tracker</name></property>
        <property><name>name_node</name></property>
        <property><name>queue_name</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive_metastore_uri</name></property>
        <property><name>oozie_launcher_queue_name</name></property>
        <property><name>oozie_launcher_memory</name></property>
        <property><name>oozie_spark_lib</name></property>
        <property><name>spark_master</name></property>
        <property><name>spark_deploy_mode</name></property>
        <property><name>spark_job_name</name></property>
        <property><name>spark_job_class</name></property>
        <property><name>spark_job_jar</name></property>
        <property><name>spark_driver_memory</name></property>
        <property><name>spark_executor_memory</name></property>
        <property><name>spark_executor_memory_overhead</name></property>
        <property><name>spark_executor_cores</name></property>
        <property><name>spark_max_num_executors</name></property>
        <property><name>spark_temp_partitions</name></property>
        <property><name>input_base_path</name></property>
        <property><name>temporary_directory</name></property>
        <property><name>output_base_path</name></property>
        <property><name>send_error_email_workflow_file</name></property>

        <property>
            <name>oozie.action.sharelib.for.spark</name>
            <value>${oozie_spark_lib}</value>
        </property>
    </parameters>

    <credentials>
        <credential name="hcat-cred" type="hcat">
            <property>
                <name>hcat.metastore.principal</name>
                <value>${hive_principal}</value>
            </property>
            <property>
               <name>hcat.metastore.uri</name>
               <value>${hive_metastore_uri}</value>
            </property>
        </credential>
    </credentials>


    <start to="generate_dumps"/>

    <action name="generate_dumps" cred="hcat-cred">
        <spark xmlns="uri:oozie:spark-action:0.1">
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <configuration>
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>
            </configuration>
            <master>${spark_master}</master>
            <mode>${spark_deploy_mode}</mode>
            <name>${spark_job_name}-${year}-${month}</name>
            <class>${spark_job_class}</class>
            <jar>${spark_job_jar}</jar>
            <!--
              NOTE: Please note the setting to make the spark job write data
                    accessible by ALL (by default, accessible only to user and group)
                    since this data is meant to be synced to labstore.
            -->
            <spark-opts>--queue ${queue_name} --driver-memory ${spark_driver_memory} --executor-memory ${spark_executor_memory} --executor-cores ${spark_executor_cores} --conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.dynamicAllocation.maxExecutors=${spark_max_num_executors} --conf spark.executor.memoryOverhead=${spark_executor_memory_overhead} --conf spark.yarn.maxAppAttempts=1 --conf spark.hadoop.fs.permissions.umask-mode=022</spark-opts>
            <arg>--input-base-path</arg>
            <arg>${input_base_path}</arg>
            <arg>--temp-directory</arg>
            <arg>${temporary_directory}/mediawiki_history_dumps_${wf:id()}</arg>
            <arg>--output-base-path</arg>
            <arg>${output_base_path}</arg>
            <arg>--temp-partitions</arg>
            <arg>${spark_temp_partitions}</arg>
            <arg>--snapshot</arg>
            <arg>${year}-${month}</arg>
        </spark>
        <ok to="end" />
        <error to="send_error_email"/>
    </action>

    <action name="send_error_email">
        <sub-workflow>
            <app-path>${send_error_email_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>parent_name</name>
                    <value>${wf:name()}</value>
                </property>
                <property>
                    <name>parent_failed_action</name>
                    <value>${wf:lastErrorNode()}</value>
                </property>
                <property>
                    <name>parent_error_code</name>
                    <value>${wf:errorCode(wf:lastErrorNode())}</value>
                </property>
                <property>
                    <name>parent_error_message</name>
                    <value>${wf:errorMessage(wf:lastErrorNode())}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="kill"/>
        <error to="kill"/>
    </action>

    <kill name="kill">
        <message>${wf:errorMessage(wf:lastErrorNode())}</message>
    </kill>

    <end name="end"/>
</workflow-app>
