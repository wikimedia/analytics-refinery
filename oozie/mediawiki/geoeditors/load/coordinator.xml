<?xml version="1.0" encoding="UTF-8"?>
<coordinator-app xmlns="uri:oozie:coordinator:0.4"
    xmlns:sla="uri:oozie:sla:0.2"
    name="mediawiki-geoeditors-load-coord"
    frequency="${coord:months(1)}"
    start="${start_time}"
    end="${stop_time}"
    timezone="Universal">

    <parameters>
        <!-- Required properties -->
        <property><name>queue_name</name></property>
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive2_jdbc_url</name></property>
        <property><name>workflow_file</name></property>
        <property><name>start_time</name></property>
        <property><name>stop_time</name></property>

        <property><name>datasets_raw_private_file</name></property>
        <property><name>mw_raw_private_directory</name></property>

        <property><name>mw_cu_changes_table</name></property>

        <property><name>hive_site_xml</name></property>
        <property><name>repair_partitions_workflow_file</name></property>
        <property><name>mark_directory_done_workflow_file</name></property>
        <property><name>send_error_email_workflow_file</name></property>

        <property><name>sla_alert_contact</name></property>
    </parameters>

    <controls>
        <!--(timeout is measured in minutes)-->
        <timeout>-1</timeout>

        <!-- Setting low concurrency for resource sharing.
             The job runs pretty fast (~1 minute) and increasing concurrency should not cause any problems-->
        <concurrency>1</concurrency>

        <throttle>2</throttle>

    </controls>

    <datasets>
        <!--
        Include raw private datasets files.
        -->
        <include>${datasets_raw_private_file}</include>
    </datasets>

    <input-events>
        <data-in name="mw_cu_changes_table" dataset="mw_cu_changes_table">
            <instance>${coord:current(0)}</instance>
        </data-in>
    </input-events>

    <output-events>
        <!-- Output partitioned datasets -->
        <data-out name="mw_cu_changes_table_partitioned" dataset="mw_cu_changes_table_partitioned">
            <instance>${coord:current(0)}</instance>
        </data-out>
    </output-events>

    <action>
        <workflow>
            <app-path>${workflow_file}</app-path>
            <configuration>
                <property>
                    <name>month</name>
                    <value>${coord:formatTime(coord:nominalTime(), "yyyy")}-${coord:formatTime(coord:nominalTime(), "MM")}</value>
                </property>
                <property>
                    <name>mw_cu_changes_table_location</name>
                    <value>${coord:dataIn('mw_cu_changes_table')}</value>
                </property>
            </configuration>
        </workflow>

        <sla:info>
            <!--
                Use action actual time as SLA base, since it's the time used
                to compute timeout
                Job is waiting for the month data to be present, so it waits ~31 days,
                then the sqoop happens on the 2nd, so waiting for 3 days after
                (31 + 3 = 34 days) should be enough.
            -->
            <sla:nominal-time>${coord:actualTime()}</sla:nominal-time>
            <sla:should-end>${34 * DAYS}</sla:should-end>
            <sla:alert-events>end_miss</sla:alert-events>
            <sla:alert-contact>${sla_alert_contact}</sla:alert-contact>
        </sla:info>

    </action>
</coordinator-app>
