<?xml version="1.0" encoding="UTF-8"?>
<workflow-app xmlns="uri:oozie:workflow:0.4"
    name="webrequest-load-wf-${webrequest_source}-${year}-${month}-${day}-${hour}">

    <parameters>

        <!-- Default values for inner oozie settings -->
        <property>
            <name>oozie_launcher_queue_name</name>
            <value>${queue_name}</value>
        </property>
        <property>
            <name>oozie_launcher_memory</name>
            <value>2048</value>
        </property>

        <!-- Required properties -->
        <property><name>queue_name</name></property>
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive2_jdbc_url</name></property>

        <property>
            <name>add_partition_workflow_file</name>
            <description>Workflow definition for adding a partition</description>
        </property>
        <property>
            <name>hive_site_xml</name>
            <description>hive-site.xml file path in HDFS</description>
        </property>
        <property>
            <name>refinery_jar_version</name>
            <description>Version of the refinery-hive jar file to import for UDFs</description>
        </property>
        <property>
            <name>artifacts_directory</name>
            <description>Path in HDFS to artifacts. refinery-hive.jar should be here.</description>
        </property>
        <property>
            <name>webrequest_raw_table</name>
            <description>Raw webrequests hive table</description>
        </property>
        <property>
            <name>webrequest_table</name>
            <description>Webrequests hive table</description>
        </property>
        <property>
            <name>webrequest_source</name>
            <description>The partition's webrequest_source</description>
        </property>
        <property>
            <name>year</name>
            <description>The partition's year</description>
        </property>
        <property>
            <name>month</name>
            <description>The partition's month</description>
        </property>
        <property>
            <name>day</name>
            <description>The partition's day</description>
        </property>
        <property>
            <name>hour</name>
            <description>The partition's hour</description>
        </property>
        <property>
            <name>record_version</name>
            <description>The record_version at the given moment</description>
        </property>
        <property>
            <name>webrequest_raw_location</name>
            <description>HDFS path(s) naming the raw webrequest dataset</description>
        </property>
        <property>
            <name>webrequest_location</name>
            <description>HDFS path(s) naming the webrequest dataset</description>
        </property>
        <property>
            <name>statistics_table</name>
            <description>
                Hive table to write partition statistics to.
            </description>
        </property>
        <property>
            <name>statistics_hourly_table</name>
            <description>
                Hive table to write hourly aggregate partition statistics to.
            </description>
        </property>
        <property>
            <name>data_loss_check_directory_base</name>
            <description>
                Base directory in HDFS where to put data loss information
            </description>
        </property>
        <property>
            <name>error_data_loss_threshold</name>
            <description>If data loss (percent) is above this value
                the job is failed and refined not launched.
            </description>
        </property>
        <property>
            <name>warning_data_loss_threshold</name>
            <description>If data loss (percent) is above this value
                and less than error_data_loss_threshold an email is sent
                but refine is still launched.
            </description>
        </property>
        <property>
            <name>mark_directory_done_workflow_file</name>
            <description>Workflow for marking a directory done</description>
        </property>
        <property>
            <name>send_error_email_workflow_file</name>
            <description>Workflow for sending an error email</description>
        </property>
    </parameters>

    <credentials>
        <credential name="hive2-cred" type="hive2">
            <property>
                <name>hive2.server.principal</name>
                <value>${hive_principal}</value>
            </property>
            <property>
               <name>hive2.jdbc.url</name>
               <value>${hive2_jdbc_url}</value>
            </property>
        </credential>
    </credentials>


    <start to="add_partition"/>

    <action name="add_partition">
        <sub-workflow>
            <app-path>${add_partition_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>table</name>
                    <value>${webrequest_raw_table}</value>
                </property>
                <property>
                    <name>location</name>
                    <value>${webrequest_raw_location}</value>
                </property>
                <property>
                    <name>partition_spec</name>
                    <value>webrequest_source='${webrequest_source}',year=${year},month=${month},day=${day},hour=${hour}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="mark_add_partition_done"/>
        <error to="send_error_email"/>
    </action>

    <!--
    This adds an empty _PARTITIONED done-flag file into
    The directory for which we just added a Hive partition.
    The webrequest_*_raw_partitioned dataset uses this.
    -->
    <action name="mark_add_partition_done">
        <sub-workflow>
            <app-path>${mark_directory_done_workflow_file}</app-path>
            <configuration>
                <property>
                    <name>directory</name>
                    <value>${webrequest_raw_location}</value>
                </property>
                <property>
                    <name>done_file</name>
                    <value>_PARTITIONED</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="generate_sequence_statistics"/>
        <error to="send_error_email"/>
    </action>

    <action name="generate_sequence_statistics" cred="hive2-cred">
        <hive2 xmlns="uri:oozie:hive2-action:0.2">
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <job-xml>${hive_site_xml}</job-xml>
            <configuration>
                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>
            </configuration>
            <jdbc-url>${hive2_jdbc_url}</jdbc-url>
            <script>generate_sequence_statistics.hql</script>

            <!-- Query parameters -->
            <param>source_table=${webrequest_raw_table}</param>
            <param>destination_table=${statistics_table}</param>
            <param>year=${year}</param>
            <param>month=${month}</param>
            <param>day=${day}</param>
            <param>hour=${hour}</param>
            <param>webrequest_source=${webrequest_source}</param>

            <!-- Beeline arguments -->
            <argument>--verbose</argument>
            <argument>--hiveconf</argument>
            <argument>mapreduce.job.queuename=${queue_name}</argument>
        </hive2>
        <ok to="generate_sequence_statistics_hourly"/>
        <error to="send_error_email"/>
    </action>


    <!-- TODO: Use the result of the hourly query to mark a dataset as _SUCCEEDED -->
    <action name="generate_sequence_statistics_hourly" cred="hive2-cred">
        <hive2 xmlns="uri:oozie:hive2-action:0.2">
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <job-xml>${hive_site_xml}</job-xml>
            <configuration>
                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>
            </configuration>
            <jdbc-url>${hive2_jdbc_url}</jdbc-url>
            <script>generate_sequence_statistics_hourly.hql</script>

            <!-- Query parameters -->
            <param>source_table=${statistics_table}</param>
            <param>destination_table=${statistics_hourly_table}</param>
            <param>year=${year}</param>
            <param>month=${month}</param>
            <param>day=${day}</param>
            <param>hour=${hour}</param>
            <param>webrequest_source=${webrequest_source}</param>

            <!-- Beeline arguments -->
            <argument>--verbose</argument>
            <argument>--hiveconf</argument>
            <argument>mapreduce.job.queuename=${queue_name}</argument>
        </hive2>
        <ok to="check_sequence_statistics"/>
        <error to="send_error_email"/>
    </action>

    <!-- We put checking sequence statistics into a separate workflow,
        as that allows to build the full path for the "faulty hosts"
        directory (including source, year, ...) as element value, and
        thereby allows to evade building it in EL via a nested concat
        construct to use it in <case />, which would not be readable
        at all. -->
    <action name="check_sequence_statistics">
        <sub-workflow>
            <app-path>${replaceAll(wf:appPath(), '/[^/]*$', '')}/check_sequence_statistics_workflow.xml</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>error_data_loss_directory</name>
                    <value>${data_loss_check_directory_base}/${webrequest_source}/${year}/${month}/${day}/${hour}/ERROR</value>
                </property>
                <property>
                    <name>warning_data_loss_directory</name>
                    <value>${data_loss_check_directory_base}/${webrequest_source}/${year}/${month}/${day}/${hour}/WARNING</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="mark_raw_dataset_done"/>
        <!-- The subworkflow sends email in case of error, not duplicating -->
        <error to="kill"/>
    </action>

    <action name="mark_raw_dataset_done">
        <sub-workflow>
            <app-path>${mark_directory_done_workflow_file}</app-path>
            <configuration>
                <property>
                    <name>directory</name>
                    <value>${webrequest_raw_location}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="refine"/>
        <error to="send_error_email"/>
    </action>

    <action name="refine" cred="hive2-cred">
        <hive2 xmlns="uri:oozie:hive2-action:0.2">
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <job-xml>${hive_site_xml}</job-xml>
            <configuration>
                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>
                <property>
                    <name>hive.exec.scratchdir</name>
                    <value>/tmp/hive-webrequest-refine</value>
                </property>
            </configuration>
            <jdbc-url>${hive2_jdbc_url}</jdbc-url>
            <script>refine_webrequest.hql</script>

            <!-- Query parameters -->
            <param>refinery_jar_version=${refinery_jar_version}</param>
            <param>artifacts_directory=${artifacts_directory}</param>
            <param>source_table=${webrequest_raw_table}</param>
            <param>destination_table=${webrequest_table}</param>
            <param>webrequest_source=${webrequest_source}</param>
            <param>record_version=${record_version}</param>
            <param>year=${year}</param>
            <param>month=${month}</param>
            <param>day=${day}</param>
            <param>hour=${hour}</param>

            <!-- Beeline arguments -->
            <argument>--verbose</argument>
            <argument>--hiveconf</argument>
            <argument>mapreduce.job.queuename=${queue_name}</argument>
        </hive2>

        <ok to="mark_refined_dataset_done"/>
        <error to="send_error_email"/>
    </action>

    <action name="mark_refined_dataset_done">
        <sub-workflow>
            <app-path>${mark_directory_done_workflow_file}</app-path>
            <configuration>
                <property>
                    <name>directory</name>
                    <value>${webrequest_location}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="end"/>
        <error to="send_error_email"/>
    </action>

    <action name="send_error_email">
        <sub-workflow>
            <app-path>${send_error_email_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>parent_id</name>
                    <value>${wf:id()}</value>
                </property>
                <property>
                    <name>parent_name</name>
                    <value>${wf:name()}</value>
                </property>
                <property>
                    <name>parent_failed_action</name>
                    <value>${wf:lastErrorNode()}</value>
                </property>
                <property>
                    <name>parent_error_code</name>
                    <value>${wf:errorCode(wf:lastErrorNode())}</value>
                </property>
                <property>
                    <name>parent_error_message</name>
                    <value>${wf:errorMessage(wf:lastErrorNode())}</value>
                </property>
                <property>
                    <name>hue_url</name>
                    <value>https://hue.wikimedia.org/oozie/list_oozie_workflow/${wf:id()}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="kill"/>
        <error to="kill"/>
    </action>

    <kill name="kill">
        <message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name="end"/>
</workflow-app>
