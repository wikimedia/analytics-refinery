<?xml version="1.0" encoding="UTF-8"?>
<!--TODO Deprecated, this job has been migrated to Airflow. -->
<workflow-app xmlns="uri:oozie:workflow:0.4"
    name="clickstream-wf-${year}-${month}">

    <parameters>
        <!-- Allows changing spark version to be used by oozie -->
        <property>
            <name>oozie.action.sharelib.for.spark</name>
            <value>${oozie_spark_lib}</value>
        </property>

        <!-- Default values for inner oozie settings -->
        <property>
            <name>oozie_launcher_queue_name</name>
            <value>${queue_name}</value>
        </property>
        <property>
            <name>oozie_launcher_memory</name>
            <value>2048</value>
        </property>

        <!-- Required properties -->
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>hive_principal</name></property>
        <property><name>hive_metastore_uri</name></property>
        <property><name>queue_name</name></property>

        <property>
            <name>spark_app_jar</name>
            <description>Path to the jar to be used to run spark application</description>
        </property>
        <property>
            <name>spark_app_class</name>
            <description>Class of the spark application to be run</description>
        </property>
        <property>
          <name>spark_app_name</name>
          <description>The spark application name</description>
        </property>
        <property>
            <name>spark_assembly_zip</name>
            <description>The assembly zip path for spark</description>
        </property>
        <property>
            <name>spark_executor_cores</name>
            <description>CPU cores to allocate for each spark executor</description>
        </property>
        <property>
            <name>spark_executor_memory</name>
            <description>Memory to allocate for each spark executor</description>
        </property>
        <property>
            <name>spark_executor_memory_overhead</name>
            <description>Memory-overhead to allocate for each spark executor</description>
        </property>
        <property>
            <name>spark_driver_memory</name>
            <description>Memory to allocate for spark driver process</description>
        </property>
        <property>
            <name>spark_max_executors</name>
            <description>Maximum concurrent number of executors for spark dynamic allocation</description>
        </property>

        <property>
          <name>pageview_actor_table</name>
          <description>The pageview_actor table</description>
        </property>
        <property>
          <name>mw_project_namespace_map_table</name>
          <description>The mediawiki project namespace map table</description>
        </property>
        <property>
          <name>mw_page_table</name>
          <description>The mediawiki page table</description>
        </property>
        <property>
          <name>mw_pagelinks_table</name>
          <description>The mediawiki pagelinks table</description>
        </property>
        <property>
          <name>mw_redirect_table</name>
          <description>The mediawiki redirect table</description>
        </property>
        <property>
            <name>year</name>
            <description>The pageview_actor partition's year</description>
        </property>
        <property>
            <name>month</name>
            <description>The pageview_actor partition's month</description>
        </property>
        <property>
            <name>snapshot</name>
            <description>The snapshot to use for mediawiki raw data(YYYY-MM)</description>
        </property>
        <property>
            <name>clickstream_wikis</name>
            <description>Wiki projects to be worked (comma separated wiki_db list)</description>
        </property>
        <property>
            <name>clickstream_minimum_links</name>
            <description>Minimum number of times a link needs to have been followed to be in the resulting dataset</description>
        </property>
        <property>
            <name>temporary_directory</name>
            <description>Temporary folder basepath where to output datasets</description>
        </property>
        <property>
            <name>clickstream_archive_base_path</name>
            <description>Basepath where to archive datasets</description>
        </property>

        <property>
            <name>loop_workflow_file</name>
            <description>Workflow looping over list elements executing another oozie action</description>
        </property>
        <property>
            <name>loop_workflow_base_path</name>
            <description>Base path for loop workflow -- needed for loop to work</description>
        </property>
        <property>
            <name>loop_archive_wrapper_workflow_file</name>
            <description>Workflow wrapping the archive worflow to pass loop_value in path</description>
        </property>
        <property>
            <name>archive_job_output_workflow_file</name>
            <description>Workflow for archiving result files</description>
        </property>
        <property>
            <name>send_error_email_workflow_file</name>
            <description>Workflow for sending an email</description>
        </property>
    </parameters>

    <credentials>
        <credential name="hcat-cred" type="hcat">
            <property>
                <name>hcat.metastore.principal</name>
                <value>${hive_principal}</value>
            </property>
            <property>
               <name>hcat.metastore.uri</name>
               <value>${hive_metastore_uri}</value>
            </property>
        </credential>
    </credentials>


    <start to="generate_clickstream_datasets"/>

    <action name="generate_clickstream_datasets" cred="hcat-cred">
        <spark xmlns="uri:oozie:spark-action:0.1">

            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <configuration>
                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>
            </configuration>
            <master>${spark_master}</master>
            <mode>${spark_deploy}</mode>
            <name>${spark_app_name}-${year}-${month}</name>
            <class>${spark_app_class}</class>
            <jar>${spark_app_jar}</jar>
            <spark-opts>--conf spark.yarn.archive=${spark_assembly_zip} --conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.yarn.executor.memoryOverhead=${spark_executor_memory_overhead} --conf spark.dynamicAllocation.maxExecutors=${spark_max_executors} --executor-cores ${spark_executor_cores} --executor-memory ${spark_executor_memory} --driver-memory ${spark_driver_memory} --queue ${queue_name}</spark-opts>
            <arg>--snapshot</arg>
            <arg>${snapshot}</arg>
            <arg>--year</arg>
            <arg>${year}</arg>
            <arg>--month</arg>
            <arg>${month}</arg>
            <arg>--minimum-count</arg>
            <arg>${clickstream_minimum_links}</arg>
            <arg>--output-base-path</arg>
            <arg>${temporary_directory}/clickstream-${wf:id()}</arg>
            <arg>--wikis</arg>
            <arg>${clickstream_wikis}</arg>
            <arg>--pageview_actor-table</arg>
            <arg>${pageview_actor_table}</arg>
            <arg>--project-namespace-table</arg>
            <arg>${mw_project_namespace_map_table}</arg>
            <arg>--page-table</arg>
            <arg>${mw_page_table}</arg>
            <arg>--redirect-table</arg>
            <arg>${mw_redirect_table}</arg>
            <arg>--pagelinks-table</arg>
            <arg>${mw_pagelinks_table}</arg>
        </spark>
        <ok to="archive_loop" />
        <error to="send_error_email" />
    </action>

    <action name="archive_loop">
        <sub-workflow>
            <app-path>${loop_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>loop_action</name>
                    <value>${loop_archive_wrapper_workflow_file}</value>
                </property>
                <property>
                    <name>loop_parallel</name>
                    <value>true</value>
                </property>
                <property>
                    <name>loop_type</name>
                    <value>list</value>
                </property>
                <property>
                    <name>loop_list</name>
                    <value>${clickstream_wikis}</value>
                </property>
                <property>
                    <name>loop_name</name>
                    <value>clickstream_archive</value>
                </property>
                <property>
                    <name>source_directory_base</name>
                    <value>${temporary_directory}/clickstream-${wf:id()}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="end"/>
        <error to="send_error_email"/>
    </action>

    <action name="send_error_email">
        <sub-workflow>
            <app-path>${send_error_email_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>parent_name</name>
                    <value>${wf:name()}</value>
                </property>
                <property>
                    <name>parent_failed_action</name>
                    <value>${wf:lastErrorNode()}</value>
                </property>
                <property>
                    <name>parent_error_code</name>
                    <value>${wf:errorCode(wf:lastErrorNode())}</value>
                </property>
                <property>
                    <name>parent_error_message</name>
                    <value>${wf:errorMessage(wf:lastErrorNode())}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="kill"/>
        <error to="kill"/>
    </action>

    <kill name="kill">
        <message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name="end"/>
</workflow-app>
